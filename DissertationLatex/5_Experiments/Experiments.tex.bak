\chapter{Experimental results}
The experiments were ran on a computer equipped with a 3.6 GHz Intel Xeon processor and a 24 GB RAM.
\section{Context Dependent Spectral Unmixing}
In this section, we present the results of the proposed CDSU algorithm on synthetic and real data sets. We also provide a comparison to the results of the P-COMMEND algorithm. The parameters of both algorithms were tuned for better results. Future work will investigate proper choices of the parameters.
\subsection{Synthetic data}
In this subsection, we present the results of our proposed approach on the three toy data sets of section \ref{motivation}. We then apply our method to simulated hyperspectral data. Due to the simplicity of the toy data sets, the performance of the algorithms is evaluated visually. However, for the simulated data set, the performance is evaluated quantitatively.
%\subsubsection{Example 1}

First, we use the same 2-dimensional data set as in section \ref{Example1Motiv} (displayed in figure \ref{figExp1}(\subref{dataExp1})). The result of the CDSU algorithm on this data set, using $C=1$, $M=3$, $m=2$, $\alpha=1$, and $\beta=0.01$ is shown in figure \ref{CDSUExp1}, where the detected endmembers are shown in red.
\begin{figure}%[htb]
  \centering
  \includegraphics[width=0.49\linewidth]{Example1CDSU.png}
\caption{Result of the CDSU algorithm on a sample 2-D synthetic data with one convex hull.}
\label{CDSUExp1}
\end{figure}
As it can be seen, the CDSU algorithm succeeded in identifying endmembers at the vertices of the convex hull enclosing the data points providing a tight fit around them.
%\subsubsection{Example 2}

As a second example, we use the same 2-dimensional data set as in section \ref{Example2Motiv} (displayed in figure \ref{figExp2}(\subref{dataExp2})). The result of the CDSU algorithm on this data set, using $C=2$, $M=3$, $m=2$, $\alpha=20$, and $\boldsymbol\beta=[0.01, 0.01]$ (same as the value of $\alpha$ used for P-COMMEND) is shown in figure \ref{CDSUExp2}. The detected endmembers are shown in red for one cluster and in green for the other cluster.
\begin{figure}%[htb]
  \centering
  \includegraphics[width=0.49\linewidth]{Example2CDSU.png}
  \caption{Result of the CDSU algorithm on a sample 2-D synthetic data with two convex hulls.}
  \label{CDSUExp2}
\end{figure}
As it can be seen, the CDSU algorithm succeeded in identifying endmember sets at the vertices of the convex hulls enclosing the data points and thus, providing a tight fit around them.
%\subsubsection{Example 3}

For a third example, we use the same 2-dimensional data set as in section \ref{Example3Motiv} (displayed in figure \ref{figExp3}(\subref{TrueAssigExp3})). The result of the CDSU algorithm on this data set, using $C=3$, $M=3$, $m=2$, $\alpha=50$, and $\boldsymbol\beta=[0.1, 0.1, 0.1]$ (same as the value of $\alpha$ used for P-COMMEND) is shown in figure \ref{CDSUExp3}. The same initializations of $\mathbf{U}$ and $\mathbf{E}_{i}$ as for P-COMMEND were used. The identified clusters are represented using different colors, and the endmembers for each cluster are represented using bolder points.
\begin{figure}%[htb]
  \centering
  \includegraphics[width=0.49\linewidth]{Example3CDSU.png}
  \caption{Result of the CDSU algorithm on a sample 2-D synthetic data with three convex hulls.}
  \label{CDSUExp3}
\end{figure}
As it can be seen, the CDSU algorithm succeeded in identifying endmember sets at the vertices of the convex hulls enclosing the data points, providing a tight fit around them. It also provided clusters that match the distribution of the data.
%\subsubsection{Simulated hyperspectral data}

For a forth example, we consider a simulated hyperspectral data set. The data is generated using the convex geometry model in (\ref{convmodl}) with six endmember signatures, selected from the USGS (United States Geological Survey) digital spectral library \cite{75}. The spectral library contains spectra of 423 minerals, 17 plants and some miscellaneous materials. In our experiment, we use spectra of the minerals Spessartine, Halloysite, Chlorite, Rectorite, Lizardite and Kaolinite, shown in figure \ref{minerals}, to generate 1000 spectral signatures. The spectra have 224 spectral bands, spanning the 0.383 - 2.508 $\mu m$ wavelength range.
%Sodium Bicarbonate, Gypsum, Chlorite, Quartz, Limonite and Kaolinite,
\begin{figure}%[htb]
  \centering
  \includegraphics[width=1\linewidth]{minerals.png}
  \caption{USGS spectra used to generate the simulated data}
  \label{minerals}
\end{figure}
Each simulated hyperspectral pixel is a convex combination of three of the endmembers, resulting in two convex regions. The proportions for each data point are generated by sampling from a standard uniform distribution and are normalized to sum to one. The first three endmembers are used to generate the first convex region, and the last three endmembers are used to generate the second one, each having 500 points. Zero-mean Gaussian noise is added to the simulated spectra at three noise levels. The noise levels are adjusted by changing the variance of the Gaussian to obtain Signal to Noise Ratios (SNR) of 20 dB, 30 dB and 50 dB  for the three levels. The SNR is defined using the logarithmic decibel scale as:
\begin{equation}\label{SNR}
    SNR=10\log_{10}\left(\frac{P_{data}}{P_{noise}}\right),
\end{equation}
where $P_{data}$ is the average power of the data, and $P_{noise}$ is the average power of the noise.
\\To evaluate the performance of the CDSU algorithm and compare it to P-COMMEND, the abundance fractions and the endmember estimates are compared with the true ones. Based on the mean square error (MSE), we define the spectral mean error ($SME$) and the abundance mean error ($AME$) as
\begin{equation}\label{SME}
    SME\equiv\frac{1}{Md}\|\mathbf{E}-\mathbf{\hat{E}}\|_{F}^{2},
\end{equation}
and
\begin{equation}\label{AME}
    AME\equiv\frac{1}{MN}\|\mathbf{P}-\mathbf{\hat{P}}\|_{F}^{2}.
\end{equation}
In (\ref{SME}) and (\ref{AME}), $M$ is the total number of endmembers, $d$ is the number of spectral bands, and $N$ is the number of data points. The rows of $\mathbf{E}$ and $\mathbf{\hat{E}}$ represent the true endmembers and the endmember estimates respectively. $\mathbf{P}$ is a $N \times M$ matrix representing the true endmember abundance fractions. $\mathbf{\hat{P}}$ is a $N \times M$ matrix representing the endmember abundance fraction estimates, where each proportion value is multiplied by the corresponding cluster membership. The notation $\|.\|_{F}$ stands for the Frobenius norm.
\\Another common performance metric is the spectral angle distance, which measures the angle between a signature $\mathbf{e}_{i}$ and its estimate $\mathbf{\hat{e}}_{i}$ \cite{1}. Based on this metric, we define a spectral mean angle error ($SMAE$) as
\begin{equation}\label{SMAE}
    SMAE\equiv\sqrt{\frac{1}{M}\sum\limits_{i=1}^{M}\left[\arccos\left(\frac{\mathbf{e}_{i}\mathbf{\hat{e}}_{i}^{T}}{\|\mathbf{e}_{i}\|\|\mathbf{\hat{e}}_{i}\|}\right)\right]^{2}}.
\end{equation}
It is clear that the performance of the algorithms increases as $SME$, $AME$, and $SMAE$ approach zero. Notice, however, that the estimates of $\mathbf{E}$ and $\mathbf{P}$  are up to a permutation matrix. Thus, a simple algorithm based on the Hungarian method \cite{76} has been designed to infer the permutation matrix.
\\We run the CDSU and P-COMMEND algorithms using the parameters in table \ref{Paramtable}.
\small
\begin{table}%[htb]
\caption{Parameters used for the CDSU and P-COMMEND algorithms on the USGS simulated hyperspectral data}
\centering
\scalebox{0.72}{
\begin{tabular}{|c|c|c|}
   \hline
   % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
   Parameters & CDSU & P-COMMEND \\\hline
   $C$ & 2 & 2 \\\hline
   $M$ & 3 & 3 \\\hline
   $m$ & 1.25 & 1.25 \\\hline
   $\alpha$, $\boldsymbol\beta$& $\alpha=200$, $\boldsymbol\beta=[0.1, 0.1]$  & $\alpha=0.1$ \\\hline
   \begin{tabular}[c]{@{}c@{}}Stopping criterion\\($Iter$ = Iteration number)\end{tabular}& $abs\left(\frac{J_{CDSU}(Iter+1)-J_{CDSU}(Iter)}{J_{CDSU}(Iter+1)}\right)<10^{-6}$&$abs\left(\frac{J_{PCOMMEND}(Iter+1)-J_{PCOMMEND}(Iter)}{J_{PCOMMEND}(Iter+1)}\right)<10^{-6}$\\\hline
 \end{tabular}
 }
\label{Paramtable}
\end{table}
\normalsize
 %$C=2$, $M=3$, $m=1.25$, $\alpha=200$, $\boldsymbol\beta=[0.1, 0.1]$ and a threshold $thresh=0.001$ for the stopping condition. Similarly, we run the P-COMMEND algorithm using $C=2$, $M=3$, $m=1.25$, $\alpha=0.1$, and a threshold $thresh=0.001$ for the stopping condition.
We run each algorithm 25 times at each noise level. For each run, we use the FCM \cite{69} and MVSA \cite{67} algorithms for the initialization of the memberships, centers and endmembers. This experiment is designed to test the sensitivity of CDSU to noise and initialization, and to provide a comparison to P-COMMEND.
%\\Table \ref{SMEtable} shows the mean and standard deviation of the endmember estimation error of both algorithms, across the 25 runs and at all noise levels, using the spectral mean error ($SME$).
%\small
%\begin{table}[htb]
%\caption{Mean and standard deviation (in brackets) of the spectral mean error ($SME$) of CDSU and P-COMMEND on the USGS simulated hyperspectral data, based on 25 runs with increasing noise and random initialization}
%\centering
%\scalebox{1}{
%\begin{tabular}{|c|c|c|}
%  \hline
%  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
%  SNR (dB) & CDSU & P-COMMEND \\\hline
%  50 & 7.3515e-004 (2.6631e-006) & 7.3072e-004 (1.3855e-006) \\\hline
%  30 &  7.5163e-004 (1.9945e-005) & 7.3564e-004 (1.6625e-005) \\\hline
%  20 &  8.5167e-004 (9.2673e-005) & 0.0013 (9.0451e-005) \\\hline
%\end{tabular}
%}
%\label{SMEtable}
%\end{table}
%\normalsize
%\\Table \ref{SMAEtable} shows the mean and standard deviation of the endmember estimation error of both algorithms, across the 25 runs and at all noise levels, using the spectral mean angle error ($SMAE$).
%\small
%\begin{table}[htb]
%\caption{Mean and standard deviation (in brackets) of the spectral mean angle error ($SMAE$) of CDSU and P-COMMEND on the USGS simulated hyperspectral data, based on 25 runs with increasing noise and random initialization}
%\centering
%\scalebox{1}{
%\begin{tabular}{|c|c|c|}
%  \hline
%  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
%  SNR (dB)& CDSU & P-COMMEND \\\hline
%  50 &  1.4049 (1.2063e-006) & 1.4049 (6.8544e-007) \\\hline
%  30 &   1.4049 (3.0583e-006) &  1.4049 (3.0746e-006) \\\hline
%  20 & 1.4050 (2.4777e-005) & 1.4050 (8.1935e-006) \\ \hline
%\end{tabular}
%}
%\label{SMAEtable}
%\end{table}
%\normalsize
%\\Table \ref{AMEtable} shows the mean and standard deviation of the abundances estimation error of both algorithms, across the 25 runs and at all noise levels, using the abundance mean error ($AME$).
%\small
%\begin{table}[htb]
%\caption{Mean and standard deviation (in brackets) of the abundance mean error ($AME$) of CDSU and P-COMMEND on the USGS simulated hyperspectral data, based on 25 runs with increasing noise and random initialization}
%\centering
%\scalebox{1}{
%\begin{tabular}{|c|c|c|}
%  \hline
%  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
%  SNR (dB)& CDSU & P-COMMEND \\\hline
%  50 &  9.0281e-004 (1.2176e-005) & 8.9104e-004 (4.6020e-006) \\\hline
%  30 &  9.8207e-004 (4.9887e-005) & 9.3822e-004 (3.6299e-005) \\\hline
%  20 & 0.0023 (5.2168e-004) &  0.0059 (5.5725e-004) \\ \hline
%\end{tabular}
%}
%\label{AMEtable}
%\end{table}
%\normalsize
\\Figures \ref{ErrorMetrics}(\subref{ErrorSME}), (\subref{ErrorSMAE}) and (\subref{ErrorAME}) show a box plot of the different error metrics of both CDSU and P-COMMEND algorithms across the 25 runs and at all noise levels. On each box, the central red mark represents the median value, the edges of the box are the $25^{\text{th}}$ and $75^{\text{th}}$ percentiles, the whiskers extend to the most extreme values not considered outliers, and outliers are plotted individually using red crosses. An outlier is a value smaller than the first quartile minus 1.5 times the interquartile range (third minus first quartile), or higher than the third quartile plus 1.5 times the interquartile range.
\begin{figure}%[h!]
\begin{subfigure}[b]{0.49\linewidth}
  \centering
  \includegraphics[width=1\linewidth]{BoxplotSMESimulated.png}
\caption{}
\label{ErrorSME}
\end{subfigure}
\begin{subfigure}[b]{0.49\linewidth}
  \centering
  \includegraphics[width=1\linewidth]{BoxplotSMAESimulated.png}
\caption{}
\label{ErrorSMAE}
\end{subfigure}

\centering
\begin{subfigure}[b]{0.49\linewidth}
  \centering
  \includegraphics[width=1\linewidth]{BoxplotAMESimulated.png}
\caption{}
\label{ErrorAME}
\end{subfigure}
\caption{Error metrics for CDSU and P-COMMEND on the USGS simulated data across the 25 runs and at all noise levels: (a) $SME$, (b) $SMAE$, (c) $AME$.}
\label{ErrorMetrics}
\end{figure}
%\\Examining these results, we see that the standard deviation of all error metrics across the 25 runs are very small, meaning that both algorithms are not sensitive to initialization, leading every time to consistent results. Also, we see that the error increases for both algorithms as the noise level increases, which is expected. Furthermore, the CDSU algorithm outperforms the P-COMMEND algorithm and this becomes more significant as the noise level increases. An explanation for this would be that CDSU is more robust to noise than P-COMMEND due to the clustering term in its objective function. Noise would not affect clustering as much as it would affect spectral unmixing.
\\Examining these results, we can see that CDSU and P-COMMEND perform similarly in low noise level cases (SNR = 30 and 50 dB). Both algorithms are also robust to initialization. For the high noise level case (SNR = 20 dB), CDSU clearly outperforms P-COMMEND ($p-value=1.4e-009$ using the Wilcoxon rank sum test \cite{wilcoxon}). An explanation for this would be that CDSU is more robust to noise than P-COMMEND due to the clustering term in its objective function. Noise would not affect clustering as much as it would affect spectral unmixing. Furthermore, we see that the error metric increases for both algorithms as the noise level increases, which is expected.
\\To compare and illustrate the results further, we visualize the estimated endmembers by both algorithms for the case of the highest noise level (SNR = 20 dB). We pick the run in which P-COMMEND gave the highest error.
Figure \ref{TESNR20}(\subref{TESNR20CDSU}) shows the true (solid lines) and estimated (dashed lines) endmembers resulting from the CDSU algorithm. Similarly, figure \ref{TESNR20}(\subref{TESNR20PCOMMEND}) shows the true and estimated endmembers resulting from the P-COMMEND algorithm. We can see that CDSU resulted in better estimates of the endmembers.
\begin{figure}%[!h]
\begin{subfigure}[b]{1\linewidth}
  \centering
  \includegraphics[width=1\linewidth, height=3.25in]{TrueEstimatedESNR20CDSU.png}
\caption{}
\label{TESNR20CDSU}
\end{subfigure}

\begin{subfigure}[b]{1\linewidth}
  \centering
  \includegraphics[width=1\linewidth, height=3.25in]{TrueEstimatedESNR20PCOMMEND.png}
\caption{}
\label{TESNR20PCOMMEND}
\end{subfigure}
\caption{True (solid lines) and estimated (dashed lines) endmembers for the USGS simulated data with SNR = 20 dB using (a) CDSU and (b) P-COMMEND.}
\label{TESNR20}
\end{figure}
\\To analyze the results further, we check the composition of the clusters generated by P-COMMEND and CDSU. The cluster assignment is based on the highest membership values. We can see, as shown in table \ref{compostable}, that both algorithms succeeded in generating clusters that are pure and that correspond to the two original convex regions.
\small
\begin{table}%[htb]
\caption{Composition of the clusters generated by CDSU and P-COMMEND on the USGS simulated data}
\centering
\scalebox{1}{
\begin{tabular}{|c|c|c|}
  \hline
  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
  & Convex set 1 & Convex set 2 \\\hline
  Cluster 1 &    500 & 0 \\\hline
  Cluster 2 &   0 & 500 \\\hline
\end{tabular}
}
\label{compostable}
\end{table}
\normalsize
\\We take the analysis one step further and we look at the membership values of the data points in each cluster. To do this, we perform a scatter plot of the two first principal components of the data and we color each point with a shade corresponding to its membership value in a specific cluster. Figures \ref{membercl1}(\subref{membercl1CDSU}) and (\subref{membercl1PCOMMEND}) show the membership values in cluster 1 generated by P-COMMEND and CDSU. Similarly, figures \ref{membercl2}(\subref{membercl2CDSU}) and (\subref{membercl2PCOMMEND}) show the membership values in cluster 2. We can see a difference in the membership values assigned by each algorithm. Compared to CDSU, P-COMMEND assigned higher membership values in cluster 1 to some points from cluster 2 (light blue shades in figure \ref{membercl2}(\subref{membercl1PCOMMEND})). And due to the sum to one constraint on the memberships, the same points were assigned lower membership values, compared to CDSU, in cluster 2 that they belong to (light green shades in figure \ref{membercl2}(\subref{membercl2PCOMMEND})). These values interfere in the update equations of the endmembers and abundances, which leads to worse estimates for P-COMMEND, and better estimates for CDSU.
\begin{figure}%[!h]
\begin{subfigure}[b]{0.49\linewidth}
  \centering
  \includegraphics[width=1\linewidth]{membershipscluster1CDSU.png}
\caption{}
\label{membercl1CDSU}
\end{subfigure}
\begin{subfigure}[b]{0.49\linewidth}
  \centering
  \includegraphics[width=1\linewidth]{membershipscluster1PCOMMEND.png}
\caption{}
\label{membercl1PCOMMEND}
\end{subfigure}
\caption{Membership values in cluster 1 for the USGS simulated data with SNR = 20 dB using (a) CDSU and (b) P-COMMEND. Two principal components of the data are scatter plotted.}
\label{membercl1}
\end{figure}
\begin{figure}%[!h]
\begin{subfigure}[b]{0.49\linewidth}
  \centering
  \includegraphics[width=1\linewidth]{membershipscluster2CDSU.png}
\caption{}
\label{membercl2CDSU}
\end{subfigure}
\begin{subfigure}[b]{0.49\linewidth}
  \centering
  \includegraphics[width=1\linewidth]{membershipscluster2PCOMMEND.png}
\caption{}
\label{membercl2PCOMMEND}
\end{subfigure}
\caption{Membership values in cluster 2 for the USGS simulated data with SNR = 20 dB using (a) CDSU and (b) P-COMMEND. Two principal components of the data are scatter plotted.}
\label{membercl2}
\end{figure}
\\We bring back the update equations of the memberships of P-COMMEND and CDSU to understand this difference. The update equation of the memberships for P-COMMEND is:
%\begin{eqnarray}\label{uPcmd2}
%   u_{ij}=\frac{\left[\frac{1}{(\mathbf{x}_{j}-\mathbf{p}_{ij}\mathbf{E}_{i})(\mathbf{x}_{j}-\mathbf{p}_{ij}\mathbf{E}_{i})^{T}}\right]^{\frac{1}{m-1}}}
%   {\sum\limits_{q=1}^{C}\left[\frac{1}{(\mathbf{x}_{j}-\mathbf{p}_{qj}\mathbf{E}_{q})(\mathbf{x}_{j}-\mathbf{p}_{qj}\mathbf{E}_{q})^{T}}\right]^{\frac{1}{m-1}}}.
%\end{eqnarray}
\begin{eqnarray}\label{uPcmd2}
   u_{ij}=\frac{\left[(\mathbf{x}_{j}-\mathbf{p}_{ij}\mathbf{E}_{i})(\mathbf{x}_{j}-\mathbf{p}_{ij}\mathbf{E}_{i})^{T}\right]^{\frac{1}{1-m}}}
   {\sum\limits_{q=1}^{C}\left[(\mathbf{x}_{j}-\mathbf{p}_{qj}\mathbf{E}_{q})(\mathbf{x}_{j}-\mathbf{p}_{qj}\mathbf{E}_{q})^{T}\right]^{\frac{1}{1-m}}}.
\end{eqnarray}
And the update equation of the memberships for CDSU, in (\ref{u}), can be rewritten as:
%\begin{equation}\label{u2}
%   u_{ij}=
%\frac{\left[\frac{1}{(\mathbf{x}_{j}-\mathbf{c}_{i})(\mathbf{x}_{j}-\mathbf{c}_{i})^{T}+\alpha (\mathbf{x}_{j}-\mathbf{p}_{ij}\mathbf{E}_{i})(\mathbf{x}_{j}-\mathbf{p}_{ij}\mathbf{E}_{i})^{T}}\right]^{\frac{1}{m-1}}}{\sum\limits_{q=1}^{C}\left[\frac{1}{(\mathbf{x}_{j}-\mathbf{c}_{q})(\mathbf{x}_{j}-\mathbf{c}_{q})^{T}+\alpha (\mathbf{x}_{j}-\mathbf{p}_{qj}\mathbf{E}_{q})(\mathbf{x}_{j}-\mathbf{p}_{qj}\mathbf{E}_{q})^{T}}\right]^{\frac{1}{m-1}}}.
%\end{equation}
\begin{equation}\label{u2}
   u_{ij}=
\frac{\left[(\mathbf{x}_{j}-\mathbf{c}_{i})(\mathbf{x}_{j}-\mathbf{c}_{i})^{T}+\alpha (\mathbf{x}_{j}-\mathbf{p}_{ij}\mathbf{E}_{i})(\mathbf{x}_{j}-\mathbf{p}_{ij}\mathbf{E}_{i})^{T}\right]^{\frac{1}{1-m}}}{\sum\limits_{q=1}^{C}\left[(\mathbf{x}_{j}-\mathbf{c}_{q})(\mathbf{x}_{j}-\mathbf{c}_{q})^{T}+\alpha (\mathbf{x}_{j}-\mathbf{p}_{qj}\mathbf{E}_{q})(\mathbf{x}_{j}-\mathbf{p}_{qj}\mathbf{E}_{q})^{T}\right]^{\frac{1}{1-m}}}.
\end{equation}
%We can see that the memberships assigned by CDSU to points from cluster $k$, in clusters $i\neq k$, are smaller than the ones assigned by P-COMMEND due to the term $(\mathbf{x}_{j}-\mathbf{c}_{i})(\mathbf{x}_{j}-\mathbf{c}_{i})^{T}$ which is high since $\mathbf{x}_{j}$ is from cluster $k$.
By examining (\ref{u2}), we notice that a pixel $j$ will have a high membership in cluster $i$ if its spectra is close to the centroid, $\mathbf{c}_{i}$, of that cluster in the feature space, and it fits the model of that cluster. This is unlike the update equation of the memberships for P-COMMEND in (\ref{uPcmd2}), where a pixel $j$ will have a high membership in model $i$ only if it fits that model, regardless of the spectral distribution around it. This shows the importance of the clustering term in taking the distribution of the data into account and thus leading to better endmember and abundance estimates. This would affect any further analysis or processing based on the estimated endmembers or abundances.
%\\To analyze the results further, we plot the spectral signatures of the clusters generated by the P-COMMEND and CDSU algorithms, labeling each point of them with the convex set it originally came from. Figures \ref{signclustsimulPCOMMEND}(\subref{signclust1simulPCOMMEND}) and (\subref{signclust2simulPCOMMEND}) show these signatures for P-COMMEND, and figures \ref{signclustsimulCDSU}(\subref{signclust1simulCDSU}) and (\subref{signclust2simulCDSU}) show them for CDSU.
%\begin{figure}[htb]
%\begin{subfigure}[b]{1\linewidth}
%  \centering
%  \includegraphics[width=1\linewidth]{cluster1PCOMMENDSimulated.png}
%  \caption{}
%  \label{signclust1simulPCOMMEND}
%\end{subfigure}
%
%\begin{subfigure}[b]{1\linewidth}
%  \centering
%  \includegraphics[width=1\linewidth]{cluster2PCOMMENDSimulated.png}
%  \caption{}
%  \label{signclust2simulPCOMMEND}
%\end{subfigure}
%\caption{Spectral signatures of the clusters generated by the P-COMMEND algorithm on the USGS simulated data: (a) Cluster 1  (b) Cluster 2.}
%\label{signclustsimulPCOMMEND}
%\end{figure}
%\begin{figure}[htb]
%\begin{subfigure}[b]{1\linewidth}
%  \centering
%  \includegraphics[width=1\linewidth]{cluster1CDSUSimulated.png}
%  \caption{}
%  \label{signclust1simulCDSU}
%\end{subfigure}
%
%\begin{subfigure}[b]{1\linewidth}
%  \centering
%  \includegraphics[width=1\linewidth]{cluster2CDSUSimulated.png}
%  \caption{}
%  \label{signclust2simulCDSU}
%\end{subfigure}
%\caption{Spectral signatures of the clusters generated by the CDSU algorithm on the USGS simulated data with SNR = 20 dB: (a) Cluster 1  (b) Cluster 2.}
%\label{signclustsimulCDSU}
%\end{figure}
%We can see that the clusters resulting from P-COMMEND are a mixture of points generated from both sets of endmembers. This mixture explains the erroneous estimates of the endmembers and abundances. However, the points of each cluster resulting from CDSU are purely generated from a separate endmember set.
%Hence, CDSU generated better clusters that took into account the spectral distribution. Clusters are more meaningful and match the distribution of the generated data. This is due to the clustering term in the objective function of CDSU.

To compare the time complexity of CDSU and P-COMMEND, we generate a box plot of the running time, expressed in seconds, across the 25 runs and at all noise levels. Both algorithms were ran using MATLAB R2011a. Figure \ref{TimeBox} shows this box plot.
\begin{figure}%[!h]
  \centering
  \includegraphics[width=0.49\linewidth]{BoxplotTimeSimulated.png}
\caption{Running time (in seconds) of CDSU and P-COMMEND on the USGS simulated data across the 25 runs and at all noise levels.}
\label{TimeBox}
\end{figure}
%\small
%\begin{table}[htb]
%\caption{Mean and standard deviation (in brackets) of the running time (in seconds) of CDSU and P-COMMEND on the USGS simulated hyperspectral data with SNR = 20 dB, based on 25 runs with increasing noise and random initialization}
%\centering
%\scalebox{1}{
%\begin{tabular}{|c|c|c|}
%  \hline
%  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
%  SNR (dB)& CDSU & P-COMMEND \\\hline
%  50 &    8.7558 (2.4633) & 6.7098 (1.6729) \\\hline
%  30 &   5.1875 (0.8276) & 4.4443 (0.9310) \\\hline
%  20 & 5.2316 (0.8026) & 5.9955 (0.5544) \\\hline
%\end{tabular}
%}
%\label{timetable}
%\end{table}
%\normalsize
%First, we notice that the running time decreases as the noise level increases for both algorithms. This is due to the fact that a noisy data requires less iterations and thus, less time to convergence than a cleaner one. Second,
We notice that the CDSU algorithm takes a longer time to run compared to P-COMMEND for the cases where SNR = 30 or 50 dB. The reason for this is that CDSU requires more computations than P-COMMEND. For the SNR = 20 dB case, we see that CDSU becomes slightly faster than P-COMMEND. This can be explained by the clustering term in CDSU which makes it more robust to noise and thus requiring less time to converge.
 %Second, in general, it takes CDSU more iterations than P-COMMEND to converge.
%\\Table \ref{iterationstable} shows the mean and standard deviation of the number of iterations needed by both algorithms to converge, across the 25 runs and at all noise levels. We can see that CDSU requires more iterations to converge than P-COMMEND. Also, this number increases as the noise level increases.
%\small
%\begin{table}[htb]
%\caption{Mean and standard deviation (in brackets) of the number of iterations needed by CDSU and P-COMMEND to converge on the USGS simulated hyperspectral data, based on 25 runs with increasing noise and random initialization}
%\centering
%\scalebox{1}{
%\begin{tabular}{|c|c|c|}
%  \hline
%  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
%  SNR (dB)& CDSU & P-COMMEND \\\hline
%  50 &  263.1600 (62.0857) &  319.8800 (81.9076) \\\hline
%  30 &  164 (27.2381) &   218.8400 (47.5085) \\\hline
%  20 &  163.1200 (24.5973) &  290.1600 (26.0171) \\  \hline
%\end{tabular}
%}
%\label{iterationstable}
%\end{table}
%\normalsize

We use the simulated data set, with SNR = 20 dB, to illustrate another advantage of our proposed approach. One might argue and say why not cluster the data into whatever number of endmember sets we are trying to find and then use a single convex region unmixing algorithm, ICE for instance, to find the endmembers and abundances for each cluster. Well, the response is that we are interested in simultaneously unmixing while taking into account the distribution of the data. This may lead to different results than if we cluster first and then unmix.
\\Figures \ref{signclust1simulFCM} and \ref{signclust2simulFCM} show the result of applying the FCM clustering algorithm to the simulated data with SNR = 20 dB. We plot the spectral signatures of the two resulting clusters, labeling each point of them with the convex set it originally came from. We clearly see that the resulting clusters include a mixture of points originally generated from both endmember sets, and hence, any unmixing algorithm applied to each cluster would result in erroneous endmember estimates. It is to note here that even if we initialize FCM with the true cluster assignments, it converges to the same result of figure \ref{signclustsimulFCM}.
\\On the other hand, CDSU resulted in pure clusters (figure \ref{signclustsimulCDSU}), even though it started with the very same erroneous FCM partition as an initialization, and hence its estimates are definitely better.
\begin{figure}%[!htb]
\begin{subfigure}[b]{1\linewidth}
  \centering
  \includegraphics[width=1\linewidth, height=3.25in]{cluster1FCMSimulated.png}
  \caption{}
  \label{signclust1simulFCM}
\end{subfigure}

\begin{subfigure}[b]{1\linewidth}
  \centering
  \includegraphics[width=1\linewidth, height=3.25in]{cluster2FCMSimulated.png}
  \caption{}
  \label{signclust2simulFCM}
\end{subfigure}
\caption{Spectral signatures of the clusters generated by the FCM algorithm on the USGS simulated data with SNR = 20 dB: (a) Cluster 1  (b) Cluster 2.}
\label{signclustsimulFCM}
\end{figure}
\begin{figure}%[!htb]
\begin{subfigure}[b]{1\linewidth}
  \centering
  \includegraphics[width=1\linewidth, height=3.25in]{cluster1CDSUSimulated.png}
  \caption{}
  \label{signclust1simulCDSU}
\end{subfigure}

\begin{subfigure}[b]{1\linewidth}
  \centering
  \includegraphics[width=1\linewidth, height=3.25in]{cluster2CDSUSimulated.png}
  \caption{}
  \label{signclust2simulCDSU}
\end{subfigure}
\caption{Spectral signatures of the clusters generated by the CDSU algorithm on the USGS simulated data with SNR = 20 dB: (a) Cluster 1  (b) Cluster 2.}
\label{signclustsimulCDSU}
\end{figure}
\\In order to understand why FCM leads to mixed clusters whereas CDSU succeeds to return pure ones, we take a closer look at the objective function values of both algorithms as they run. Figure \ref{FCMInit} shows the evolution of the objective function of FCM until its convergence. We can see that it reaches a local minimum in few iterations.
\begin{figure}%[htb]
  \centering
  \includegraphics[width=0.7\linewidth]{FCMObjInitialization.png}
\caption{Objective function of the FCM algorithm used for cluster initialization.}
\label{FCMInit}
\end{figure}
 As CDSU runs, we compute the objective function of FCM using the resulting memberships and centers from CDSU at every iteration. Figure \ref{FCMObjCDSU} shows the evolution of this objective function. We can see that, as CDSU runs, the objective function of FCM increases. This explains why FCM did not reach the optimal pure clusters. It is because the corresponding objective function's value is greater than the local minimum that FCM found during the initialization.
\begin{figure}%[htb]
  \centering
  \includegraphics[width=0.7\linewidth]{FCMobjiterCDSU1.png}
\caption{Evolution of the FCM objective function as CDSU runs.}
\label{FCMObjCDSU}
\end{figure}
\\On the other hand, figure \ref{CDSUObjCDSU} shows the evolution of the objective function of the CDSU algorithm. We can see that it decreases until it reaches a local minimum. This can be explained by the second term of the objective function corresponding to the spectral unmixing. It is the one decreasing so much that it compensates for the increase of the first term corresponding to the clustering. And hence, the overall objective function decreases.
\begin{figure}%[htb]
  \centering
  \includegraphics[width=0.7\linewidth]{CDSUobj.png}
\caption{Evolution of the CDSU objective function as CDSU runs.}
\label{CDSUObjCDSU}
\end{figure}
\\We pick few intermediary results during the run of the CDSU algorithm in order to illustrate the composition of the clusters at each iteration. The initial composition is the same as in the FCM result shown in figure \ref{signclustsimulFCM}. The composition of the clusters after the first iteration is shown in figure \ref{signclustsimulCDSUIter1}. We can see that CDSU succeeded in purifying cluster 2. It is now composed only of points generated from the second set of endmembers. Cluster 1 also got purer with only three points generated from the second set of endmembers.
\newpage
\vfill
\begin{figure}%[!htb]
\begin{subfigure}[b]{1\linewidth}
  \centering
  \includegraphics[width=1\linewidth, height=3.25in]{CDSUIter1Cluster1spectralSign.png}
  \caption{}
  \label{signclust1simulCDSUIter1}
\end{subfigure}

\begin{subfigure}[b]{1\linewidth}
  \centering
  \includegraphics[width=1\linewidth, height=3.25in]{CDSUIter1Cluster2spectralSign.png}
  \caption{}
  \label{signclust2simulCDSUIter1}
\end{subfigure}
\caption{Spectral signatures of the resulting clusters from the CDSU algorithm on the USGS simulated data with SNR = 20 dB after the first iteration: (a) Cluster 1  (b) Cluster 2.}
\label{signclustsimulCDSUIter1}
\end{figure}
\vfill
\clearpage
Figure \ref{signclustsimulCDSUIter2} shows the composition of the clusters after the second iteration. Cluster 2 is always pure, and cluster 1 now has two points only generated from the second set of endmembers.
\begin{figure}%[!htb]
\begin{subfigure}[b]{1\linewidth}
  \centering
  \includegraphics[width=1\linewidth, height=3.25in]{CDSUIter2Cluster1spectralSign.png}
  \caption{}
  \label{signclust1simulCDSUIter2}
\end{subfigure}

\begin{subfigure}[b]{1\linewidth}
  \centering
  \includegraphics[width=1\linewidth, height=3.25in]{CDSUIter2Cluster2spectralSign.png}
  \caption{}
  \label{signclust2simulCDSUIter2}
\end{subfigure}
\caption{Spectral signatures of the resulting clusters from the CDSU algorithm on the USGS simulated data with SNR = 20 dB after the second iteration: (a) Cluster 1  (b) Cluster 2.}
\label{signclustsimulCDSUIter2}
\end{figure}
\\Figure \ref{signclustsimulCDSUIter3} shows the composition of the clusters after the third iteration. Cluster 2 is always pure, and cluster 1 now has one point only generated from the second set of endmembers.
\begin{figure}%[!htb]
\begin{subfigure}[b]{1\linewidth}
  \centering
  \includegraphics[width=1\linewidth, height=3.25in]{CDSUIter3Cluster1spectralSign.png}
  \caption{}
  \label{signclust1simulCDSUIter3}
\end{subfigure}

\begin{subfigure}[b]{1\linewidth}
  \centering
  \includegraphics[width=1\linewidth, height=3.25in]{CDSUIter3Cluster2spectralSign.png}
  \caption{}
  \label{signclust2simulCDSUIter3}
\end{subfigure}
\caption{Spectral signatures of the resulting clusters from the CDSU algorithm on the USGS simulated data with SNR = 20 dB after the third iteration: (a) Cluster 1  (b) Cluster 2.}
\label{signclustsimulCDSUIter3}
\end{figure}
This result remains for six more iterations, and at the tenth iteration, both clusters get pure as shown in figure \ref{signclustsimulCDSU}. For the remaining of the iterations, the composition of the clusters do not change and the CDSU algorithm keeps updating the variables until convergence.

Using two dimensional toy data and simulated hyperspectral data, we were able to illustrate the effectiveness of the proposed CDSU algorithm in identifying correct abundances and endmember sets. We also showed that CDSU outperformed P-COMMEND in the case of noisy data. Furthermore, we showed the difference between our approach, that performs spectral unmixing while simultaneously taking into account the distribution of the data in the spectral space, and simply clustering the data and then unmixing each cluster separately. \\In the next section, we present the results of CDSU on real hyperspectral data and compare them to those of P-COMMEND.
\subsection{Real data}
\label{sec:expcdsu}
In this section, we consider a real hyperspectral data set. The data was collected on July 8, 2002 over an urban area around the Pavia University in northern Italy, using the Reflective Optics System Imaging Spectrometer\footnote{Data available at http://www.ehu.es/ccwintco/index.php/Hyperspectral\_Remote\_Sensing\_Scenes} (ROSIS). The ROSIS sensor collects data over the $430-860$ nm wavelength range at a 4 nm spectral sampling interval. The image originally contains 610$\times$340 pixels having 103 spectral bands each ($430-838$ nm), with a spatial resolution of 1.3 meters. In our experiment, we used a down sampled version of the image, of size 305$\times$170. The scene consists of both natural and urban regions as shown in figure \ref{pavia}. Ground truth labels are provided for some areas of the scene. Nine classes are defined: asphalt, meadows, gravel, trees, painted metal sheets, bare soil, bitumen, self-blocking bricks and shadows. Figure \ref{GTPaviaU} shows the ground truth image of the Pavia University data set.
\begin{figure}[!h]
\centering
\includegraphics[width=0.45\linewidth, height=0.65\linewidth]{Pavia305170.png}
\caption{Image of the Pavia University data set (using bands 56, 29 and 12 to form R, G and B channels to create the color image)}
\label{pavia}
\end{figure}
\begin{figure}[!h]
\centering
\includegraphics[width=0.5\linewidth, height=0.65\linewidth]{PaviaGT305170.png}
\caption{Ground truth image of the Pavia University data set}
\label{GTPaviaU}
\end{figure}
\\We run the CDSU and P-COMMEND on this data set using the parameters in table \ref{Paramtable2}.
\small
\begin{table}[htb]
\caption{Parameters used for the CDSU and P-COMMEND algorithms on the Pavia University hyperspectral data}
\centering
\scalebox{0.75}{
\begin{tabular}{|c|c|c|}
   \hline
   % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
   Parameters & CDSU & P-COMMEND \\\hline
   $C$ & 3 & 3 \\\hline
   $M$ & 3 & 3 \\\hline
   $m$ & 1.25 & 1.25 \\\hline
   $\alpha$, $\boldsymbol\beta$& $\alpha=1$, $\boldsymbol\beta=[5, 5, 5]$  & $\alpha=5$ \\\hline
   \begin{tabular}[c]{@{}c@{}}Stopping criterion\\($Iter$ = Iteration number)\end{tabular} & $abs\left(\frac{J_{CDSU}(Iter+1)-J_{CDSU}(Iter)}{J_{CDSU}(Iter+1)}\right)<10^{-6}$&$abs\left(\frac{J_{PCOMMEND}(Iter+1)-J_{PCOMMEND}(Iter)}{J_{PCOMMEND}(Iter+1)}\right)<10^{-6}$\\\hline
 \end{tabular}
 }
\label{Paramtable2}
\end{table}
\normalsize
%Since we do not dispose of the true endmembers and proportions for this data set, we compare the performances of the algorithms qualitatively by analyzing their respective results, and quantitatively by measuring the error of reconstructing the image from the endmember and abundance estimates using the mean square error metric
%\begin{equation}\label{MSE}
%    MSE\equiv\frac{1}{Nd}\|\mathbf{X}-\mathbf{\hat{X}}\|_{F}^{2}.
%\end{equation}
%In (\ref{MSE}), $\mathbf{X}$ is the true data, and $\mathbf{\hat{X}}$ is the reconstructed data using the endmember and abundance estimates. In each context, each abundance value is multiplied by the corresponding membership value. In (\ref{MSE}), $N$ is the number of data points, $d$ is the number of spectral bands, and $\|.\|_{F}$ stands for the Frobenius norm.
\\Figures \ref{CDSUPCMDClustersPaviaU}(\subref{CDSUClustersPaviaU}) and (\subref{PCMDClustersPaviaU}) show the images of cluster assignment for both algorithms. Cluster assignments were based on the highest membership values.
\begin{figure}[!h]
\begin{subfigure}[b]{0.5\linewidth}
  \centering
\includegraphics[width=1\linewidth, height=1.3\linewidth]{CDSUClustersPavia.png}
\caption{}
  \label{CDSUClustersPaviaU}
\end{subfigure}
\begin{subfigure}[b]{0.5\linewidth}
   \centering
\includegraphics[width=1\linewidth, height=1.3\linewidth]{PCOMMENDClustersPavia.png}
\caption{}
  \label{PCMDClustersPaviaU}
\end{subfigure}
\caption{Cluster assignment images generated by (a) CDSU and (b) P-COMMEND on the Pavia University Data}
\label{CDSUPCMDClustersPaviaU}
\end{figure}
Table \ref{clustcompPCMD} shows the composition of the clusters generated by P-COMMEND, in percentage, taking into account the labeled points only.
\begin{table}
  \caption{Composition of the clusters generated by P-COMMEND, in percentage, taking into account the labeled points only}
  \centering
  \scalebox{0.7}{
  \begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
    \hline
    % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
     \backslashbox{Cluster}{Class} & Asphalt & Meadows & Gravel & Trees & Painted metal sheets & Bare soil & Bitumen & Self blocking bricks & Shadows \\\hline
    Cluster 1 & 16.01 & 22.78 & 3.19 & 35.36 & 0 & 7.14 & 0.19 & 15.30 & 0\\\hline
    Cluster 2 & 0.57 & 76.74 & 0.03 & 0.51 & 0 & 17.74 & 0.01 & 0.18 & 4.17\\\hline
    Cluster 3 & 40.89 & 0 & 14.58 & 0.03 & 10.66 & 4.71 & 10.38 & 18.66 & 0.06\\
    \hline
  \end{tabular}
  }
\label{clustcompPCMD}
\end{table}
We can see that cluster 1 is composed mostly of trees, meadows, asphalt and bricks. Cluster 2 is composed mostly of meadows, bare soil and shadows. And cluster 3 is composed mostly of asphalt, bricks, gravel, metal sheets and bitumen.
Table \ref{clustcompCDSU} shows the composition of the clusters generated by CDSU, in percentage, taking into account the labeled points only.
\begin{table}
  \caption{Composition of the clusters generated by CDSU, in percentage, taking into account the labeled points only}
  \centering
  \scalebox{0.7}{
  \begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
    \hline
    % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
   \backslashbox{Cluster}{Class}  & Asphalt & Meadows & Gravel & Trees & Painted metal sheets & Bare soil & Bitumen & Self blocking bricks & Shadows \\\hline
    Cluster 1 & 45.54 & 21.46 & 3.98 & 0.24 & 0.03 & 10.29 & 9.23 & 2.32 & 6.88\\\hline
    Cluster 2 & 0.05 & 73.08 & 0 & 14.71 & 0 & 12.14 & 0 & 0 & 0\\\hline
    Cluster 3 & 6.45 & 6.17 & 18.32 & 0.04 & 15.49 & 13.36 & 1.16 & 38.97 & 0\\
    \hline
  \end{tabular}
  }
\label{clustcompCDSU}
\end{table}
We can see that cluster 1 is composed mostly of asphalt, meadows, bare soil, bitumen and shadows. Cluster 2 is composed mostly of meadows, trees and bare soil. And cluster 3 is composed mostly of bricks, gravel, metal sheets and bare soil. This can also be seen in figures \ref{CDSUPCMDClustersPaviaUcomp}(\subref{CDSUClustersPaviaUcomp}) and (\subref{PCMDClustersPaviaUcomp}) where the same cluster compositions are represented visually.
\begin{figure}[!h]
\begin{subfigure}[b]{0.5\linewidth}
  \centering
\includegraphics[width=1\linewidth, height=1\linewidth]{CDSUClustersPaviacomp.png}
\caption{}
  \label{CDSUClustersPaviaUcomp}
\end{subfigure}
\begin{subfigure}[b]{0.5\linewidth}
   \centering
\includegraphics[width=1\linewidth, height=1\linewidth]{PCOMMENDClustersPaviacomp.png}
\caption{}
  \label{PCMDClustersPaviaUcomp}
\end{subfigure}
\caption{Composition, in percentage, of the clusters generated by (a) CDSU and (b) P-COMMEND on the Pavia University Data, taking into account the labeled points only.}
\label{CDSUPCMDClustersPaviaUcomp}
\end{figure}
Comparing the compositions of the clusters from both algorithms, we can see that some classes were divided into more than one cluster by both algorithms (meadows and bare soil). Other classes were divided into more than one cluster by P-COMMEND and less divided by CDSU (bricks and asphalt).

The proportion maps associated with the three endmembers for each of the three clusters found by P-COMMEND are shown in figures \ref{clus1paviapcmd}, \ref{clus2paviapcmd} and \ref{clus3paviapcmd}.
Since we do not have labels for the entire image, we analyze these proportion maps by comparing them with the RGB image in figure \ref{pavia}.
\\In cluster 1 (figure \ref{clus1paviapcmd}), the proportion map in figure \ref{clus1paviapcmd}(\subref{clus1em1pcmd}), corresponding to the first endmember, represents regions with high grass or trees. Similarly, we can see that the proportion map in figure \ref{clus1paviapcmd}(\subref{clus1em2pcmd}), corresponding to the second endmember in context 1, represents regions of shadow. The proportion map in figure \ref{clus1paviapcmd}(\subref{clus1em3pcmd}), corresponding to the third endmember in cluster 1, represents cement (parking lot).
\\In cluster 2 (figure \ref{clus2paviapcmd}), the proportion map in figure \ref{clus2paviapcmd}(\subref{clus2em1pcmd}), corresponding to the first endmember, represents regions of shadow. The proportion map in figure \ref{clus2paviapcmd}(\subref{clus2em2pcmd}), corresponding to the second endmember, represents regions with high grass or trees. Finally, the proportion map in figure \ref{clus2paviapcmd}(\subref{clus2em3pcmd}), corresponding to the third endmember, represents bricks (roofs).
\\In cluster 3 (figure \ref{clus3paviapcmd}), the proportion map in figure \ref{clus3paviapcmd}(\subref{clus3em1pcmd}), corresponding to the first endmember, represents cars. The proportion map in figure \ref{clus3paviapcmd}(\subref{clus3em2pcmd}), corresponding to the second endmember, represents asphalt (roads), bitumen (top of some roofs) and cement (top of some other roofs). Finally, the proportion map in figure \ref{clus3paviapcmd}(\subref{clus3em3pcmd}), corresponding to the third endmember, represents metal roofs.
\\We notice that the endmembers found by  P-COMMEND are repetitive in more than one cluster. For instance, endmembers corresponding to shadows, high grass and trees are found in cluster 1 and 2. Furthermore, the endmember for cement is found in clusters 1 and 3. Some endmembers represent non coherent elements, like combining asphalt and bitumen with cement in one endmember.
\newpage
\vfill
\begin{figure}[!htb]
\begin{subfigure}[b]{0.5\linewidth}
  \centering
  \includegraphics[width=1\linewidth,height=1.3\linewidth]{PCMDClus1E1PaviaU.png}
  \caption{}
  \label{clus1em1pcmd}
\end{subfigure}
\begin{subfigure}[b]{0.5\linewidth}
  \centering
  \includegraphics[width=1\linewidth,height=1.3\linewidth]{PCMDClus1E2PaviaU.png}
  \caption{}
  \label{clus1em2pcmd}
\end{subfigure}

\begin{subfigure}[b]{1\linewidth}
  \centering
  \includegraphics[width=0.5\linewidth,height=0.65\linewidth]{PCMDClus1E3PaviaU.png}
  \caption{}
  \label{clus1em3pcmd}
\end{subfigure}
\caption{Proportion maps for cluster 1 estimated by the P-COMMEND algorithm from the Pavia University hyperspectral data. Each pixel in the proportion maps was multiplied by the corresponding membership value for each context such that the endmembers with high proportion for each data point are highlighted.}
\label{clus1paviapcmd}
\end{figure}
\vfill
\clearpage
\newpage
\vfill
\begin{figure}[!htb]
\begin{subfigure}[b]{0.5\linewidth}
  \centering
  \includegraphics[width=1\linewidth,height=1.3\linewidth]{PCMDClus2E1PaviaU.png}
  \caption{}
  \label{clus2em1pcmd}
\end{subfigure}
\begin{subfigure}[b]{0.5\linewidth}
  \centering
  \includegraphics[width=1\linewidth,height=1.3\linewidth]{PCMDClus2E2PaviaU.png}
  \caption{}
  \label{clus2em2pcmd}
\end{subfigure}

\begin{subfigure}[b]{1\linewidth}
  \centering
  \includegraphics[width=0.5\linewidth,height=0.65\linewidth]{PCMDClus2E3PaviaU.png}
  \caption{}
  \label{clus2em3pcmd}
\end{subfigure}
\caption{Proportion maps for cluster 2 estimated by the P-COMMEND algorithm from the Pavia University hyperspectral data. Each pixel in the proportion maps was multiplied by the corresponding membership value for each context such that the endmembers with high proportion for each data point are highlighted.}
\label{clus2paviapcmd}
\end{figure}
\vfill
\clearpage
\newpage
\vfill
\begin{figure}[!htb]
\begin{subfigure}[b]{0.5\linewidth}
  \centering
  \includegraphics[width=1\linewidth,height=1.3\linewidth]{PCMDClus3E1PaviaU.png}
  \caption{}
  \label{clus3em1pcmd}
\end{subfigure}
\begin{subfigure}[b]{0.5\linewidth}
  \centering
  \includegraphics[width=1\linewidth,height=1.3\linewidth]{PCMDClus3E2PaviaU.png}
  \caption{}
  \label{clus3em2pcmd}
\end{subfigure}

\begin{subfigure}[b]{1\linewidth}
  \centering
  \includegraphics[width=0.5\linewidth,height=0.65\linewidth]{PCMDClus3E3PaviaU.png}
  \caption{}
  \label{clus3em3pcmd}
\end{subfigure}

\caption{Proportion maps for cluster 3 estimated by the P-COMMEND algorithm from the Pavia University hyperspectral data. Each pixel in the proportion maps was multiplied by the corresponding membership value for each context such that the endmembers with high proportion for each data point are highlighted.}
\label{clus3paviapcmd}
\end{figure}
\vfill
\clearpage
%The image reconstructed using the endmember and abundance estimates of P-COMMEND is illustrated in figure \ref{paviareconspcmd}.
%\begin{figure}[!h]
%\centering
%\includegraphics[width=0.45\linewidth, height=0.65\linewidth]{Pavia305170reconstructedpcmd.png}
%\caption{Reconstructed image of the Pavia University data set from the endmember and abundance estimates of P-COMMEND (using bands 56, 29 and 12 to form R, G and B channels to create the color image)}
%\label{paviareconspcmd}
%\end{figure}

The proportion maps associated with the three endmembers for each of the three contexts (clusters) found by CDSU are shown in figures \ref{clus1paviacdsu}, \ref{clus2paviacdsu} and \ref{clus3paviacdsu}.
Since we do not have labels for the entire image, we analyze these proportion maps by comparing them with the RGB image in figure \ref{pavia}.
\\In context 1 (figure \ref{clus1paviacdsu}), the proportion map in figure \ref{clus1paviacdsu}(\subref{clus1em1cdsu}), corresponding to the first endmember, represents asphalt (roads) and bitumen (top of some roofs). Similarly, we can see that the proportion map in figure \ref{clus1paviacdsu}(\subref{clus1em2cdsu}), corresponding to the second endmember in context 1, represents shadows. The proportion map in figure \ref{clus1paviacdsu}(\subref{clus1em3cdsu}), corresponding to the third endmember in context 1, represents uneven bare soil.
\\In context 2 (figure \ref{clus2paviacdsu}), the proportion map in figure \ref{clus2paviacdsu}(\subref{clus2em1cdsu}), corresponding to the first endmember, represents regions with high grass or trees. The proportion map in figure \ref{clus2paviacdsu}(\subref{clus2em2cdsu}), corresponding to the second endmember, represents even bare soil. Finally, the proportion map in figure \ref{clus2paviacdsu}(\subref{clus2em3cdsu}), corresponding to the third endmember, represents regions with low grass.
\\In context 3 (figure \ref{clus3paviacdsu}), the proportion map in figure \ref{clus3paviacdsu}(\subref{clus3em1cdsu}), corresponding to the first endmember, represents cars. The proportion map in figure \ref{clus3paviacdsu}(\subref{clus3em2cdsu}), corresponding to the second endmember, represents metal roofs. Finally, the proportion map in figure \ref{clus3paviacdsu}(\subref{clus3em3cdsu}), corresponding to the third endmember, represents cement (parking lots, sidewalks and roofs) and bricks (roofs).
\\We notice that the endmembers found by CDSU are not repetitive in the three contexts and that they represent coherent elements.
\newpage
\vfill
\begin{figure}[!htb]
\begin{subfigure}[b]{0.5\linewidth}
  \centering
  \includegraphics[width=1\linewidth,height=1.3\linewidth]{CDSUClus1E1PaviaU.png}
  \caption{}
  \label{clus1em1cdsu}
\end{subfigure}
\begin{subfigure}[b]{0.5\linewidth}
  \centering
  \includegraphics[width=1\linewidth,height=1.3\linewidth]{CDSUClus1E2PaviaU.png}
  \caption{}
  \label{clus1em2cdsu}
\end{subfigure}

\begin{subfigure}[b]{1\linewidth}
  \centering
  \includegraphics[width=0.5\linewidth,height=0.65\linewidth]{CDSUClus1E3PaviaU.png}
  \caption{}
  \label{clus1em3cdsu}
\end{subfigure}
\caption{Proportion maps for context 1 estimated by the CDSU algorithm from the Pavia University hyperspectral data. Each pixel in the proportion maps was multiplied by the corresponding membership value for each context such that the endmembers with high proportion for each data point are highlighted.}
\label{clus1paviacdsu}
\end{figure}
\vfill
\clearpage
\newpage
\vfill
\begin{figure}[!htb]
\begin{subfigure}[b]{0.5\linewidth}
  \centering
  \includegraphics[width=1\linewidth,height=1.3\linewidth]{CDSUClus2E1PaviaU.png}
  \caption{}
  \label{clus2em1cdsu}
\end{subfigure}
\begin{subfigure}[b]{0.5\linewidth}
  \centering
  \includegraphics[width=1\linewidth,height=1.3\linewidth]{CDSUClus2E2PaviaU.png}
  \caption{}
  \label{clus2em2cdsu}
\end{subfigure}

\begin{subfigure}[b]{1\linewidth}
  \centering
  \includegraphics[width=0.5\linewidth,height=0.65\linewidth]{CDSUClus2E3PaviaU.png}
  \caption{}
  \label{clus2em3cdsu}
\end{subfigure}
\caption{Proportion maps for context 2 estimated by the CDSU algorithm from the Pavia University hyperspectral data. Each pixel in the proportion maps was multiplied by the corresponding membership value for each context such that the endmembers with high proportion for each data point are highlighted.}
\label{clus2paviacdsu}

\end{figure}
\vfill
\clearpage
\newpage
\vfill
\begin{figure}[!htb]
\begin{subfigure}[b]{0.5\linewidth}
  \centering
  \includegraphics[width=1\linewidth,height=1.3\linewidth]{CDSUClus3E1PaviaU.png}
  \caption{}
  \label{clus3em1cdsu}
\end{subfigure}
\begin{subfigure}[b]{0.5\linewidth}
  \centering
  \includegraphics[width=1\linewidth,height=1.3\linewidth]{CDSUClus3E2PaviaU.png}
  \caption{}
  \label{clus3em2cdsu}
\end{subfigure}

\begin{subfigure}[b]{1\linewidth}
  \centering
  \includegraphics[width=0.5\linewidth,height=0.65\linewidth]{CDSUClus3E3PaviaU.png}
  \caption{}
  \label{clus3em3cdsu}
\end{subfigure}

\caption{Proportion maps for context 3 estimated by the CDSU algorithm from the Pavia University hyperspectral data. Each pixel in the proportion maps was multiplied by the corresponding membership value for each context such that the endmembers with high proportion for each data point are highlighted.}
\label{clus3paviacdsu}
\end{figure}
\vfill
\clearpage
%The image reconstructed using the endmember and abundance estimates of CDSU is illustrated in figure \ref{paviareconscdsu}.
%\begin{figure}[!h]
%\centering
%\includegraphics[width=0.45\linewidth, height=0.65\linewidth]{Pavia305170reconstructedCDSU.png}
%\caption{Reconstructed image of the Pavia University data set from the endmember and abundance estimates of CDSU (using bands 56, 29 and 12 to form R, G and B channels to create the color image)}
%\label{paviareconscdsu}
%\end{figure}
Figures \ref{EstimEPavia}(\subref{EstimEPaviaCDSU}) and (\subref{EstimEPaviaPCMD}) show the estimated endmembers by CDSU and P-COMMEND. Both algorithms agree on some endmembers like the ones corresponding to metal roofs, cars and high grass and trees, even though P-COMMEND generated a repetitive endmember for high grass and trees. P-COMMEND also generated a repetitive endmember for shadows.
\begin{figure}[!h]
\begin{subfigure}[b]{1\linewidth}
  \centering
  \includegraphics[width=1\linewidth, height=3.25in]{EstimatedEndmembersCDSU.png}
\caption{}
\label{EstimEPaviaCDSU}
\end{subfigure}

\begin{subfigure}[b]{1\linewidth}
  \centering
  \includegraphics[width=1\linewidth, height=3.25in]{EstimatedEndmembersPCMD.png}
\caption{}
\label{EstimEPaviaPCMD}
\end{subfigure}
\caption{Estimated endmembers for the Pavia University hyperspectral data using (a) CDSU and (b) P-COMMEND.}
\label{EstimEPavia}
\end{figure}

In this section, we presented the results of the proposed CDSU algorithm on real hyperspectral data and compared them to those of P-COMMEND. Both methods did agree on some endmembers. However, while P-COMMEND resulted in some repetitive and non coherent endmembers, CDSU yielded coherent and non repetitive endmembers representing meaningful elements in the hyperspectral scene. This would have an impact on any further processing or analysis based on the estimated abundances or endmembers. These may include looking up the estimated endmembers in a spectral library to identify which elements they correspond to, or using the proportions as the pixels' features for a classification task.
\section{Context Dependent Spectral Unmixing Using Gustafson-Kessel Clustering}
%In this section, we present the results of the proposed GK-CDSU algorithm on synthetic and real data sets. A comparison to CDSU is also provided.
%\subsection{Synthetic data}
We design a synthetic 2-dimensional data set that includes 2 elongated convex sets generated using the linear model of equation (\ref{convmodl}). These are illustrated in figure \ref{data2dcdsugkcdsu}. Each cluster contains 2000 points. CDSU and GK-CDSU were ran using the same parameters values: $C=2$, $M=3$, $m=2$, $\alpha=40$, and $\boldsymbol\beta=[0.2, 0.2]$. For GK-CDSU, $\sigma_{i}$ was set to $1$ for $i=1, 2$.
\begin{figure}[htb]
  \centering
  \includegraphics[width=0.49\linewidth]{data2dcdsugkcdsu.png}
\caption{2-D synthetic data with two elongated convex hulls.}
\label{data2dcdsugkcdsu}
\end{figure}

Figure \ref{cdsugkcdsuon2ddata} illustrates the results of both algorithms. The retrieved clusters are represented in different colors, and the endmembers are represented in red dots. Due to the ``non-spherical'' nature of the clusters, CDSU failed to identify the correct cluster assignments. This made it fail to identify appropriate endmembers. On the other hand, GK-CDSU successfully identified both the clusters and the endmembers.
\begin{figure}[h!]
\begin{subfigure}[b]{0.49\linewidth}
  \centering
  \includegraphics[width=1\linewidth]{cdsuon2ddata.png}
\caption{CDSU}
\label{cdsuon2ddata}
\end{subfigure}
\begin{subfigure}[b]{0.49\linewidth}
  \centering
  \includegraphics[width=1\linewidth]{gkcdsuon2ddata.png}
\caption{GK-CDSU}
\label{gkcdsuon2ddata}
\end{subfigure}
\caption{Results of CDSU and GK-CDSU on the data of figure \ref{data2dcdsugkcdsu}}
\label{cdsugkcdsuon2ddata}
\end{figure}

\section{Graph Constrained Multi-Model Unmixing}
\label{sec:gcmmu}
Two data sets are used in this experiment. The first one is a LIDAR image acquired using an Optech Inc. Gemini Airborne Topographic LIDAR Mapper (ALTM) system. The second one is a hyperspectral image acquired using an ITRES Inc. hyperspectral Compact Airborne Spectrographic Imager (CASI-1500), which measured reflectance in 68 spectral bands across the Visible and Near-Infrared (VISNIR) wavelengths. Both data sets were geometrically corrected to be co-registered with $1m \times 1m$ pixels. The images were acquired over the campus of the University of Southern Mississippi in Gulfport, Mississippi, in November 2010.

The LIDAR image is segmented using the Digital Elevation Maps (DEM) to partition the image into different elevation levels; using the vegetation index NDVI on the co-located hyperspectral image to identify vegetation regions; and using the altitude of the plane, the position of the sun at the time of the collection, and the DEM to identify shadows \cite{taylor}. Figure \ref{fig1} illustrates the RGB image of the area and the eight resulting segments of the LIDAR image.
 %0) unlabeled, 1) ground/impervious, 2) ground/pervious, 3) ground/shadow, 4) trees, 5) buildings, 6) beach and 7) tarps.
The resulting segmented image is used to construct a set of \emph{should-link} constraints by randomly selecting pairs of pixels that belong to the same segment. The image has 27547 pixels. The set of constraints includes 6840 \emph{should-link} pairs.
\begin{figure}[h!]
\begin{subfigure}[b]{0.49\linewidth}
  \centering
\includegraphics[width=1.1\linewidth]{rgb.png}%
\caption{}
  \label{rgb}
\end{subfigure}
\begin{subfigure}[b]{0.49\linewidth}
  \centering
\includegraphics[width=1.1\linewidth]{lidar.png}%
\caption{}
  \label{lidar}
\end{subfigure}
\caption{(a) RGB and (b) segmented LIDAR images of the University of Southern Mississipi.}
\label{fig1}
\end{figure}
The dimensionality of the hyperspectral data was reduced from 68 to 30 using the Ward's linkage strategy with divergence from \cite{ward}.

We run GC-MMU and compare it to CDSU using the parameters in table \ref{Paramtable3}. The off-diagonal elements of the norm matrices $\mathbf{A}_{i}$ are set to zero. This is mainly because, in a high dimensional space, cross-covariances between dimensions are not meaningful.
\begin{table}[htb]
\caption{Parameters used for the GC-MMU and CDSU algorithms}
\centering
\scalebox{1}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
   \hline
   % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
   Parameters & $C$ &$M$ &$m$&$\alpha$&$\boldsymbol\beta$&$\gamma$&$\sigma_{i},\; \forall i$&$\rho_{jk}, \; \forall j, k$\\\hline
   GC-MMU& 3 & 3 & 2& 4 &[10, 10, 10]&0.3& 1&1\\\hline
   CDSU & 3 & 3 & 2& 4&[10, 10, 10]& \multicolumn{3}{c|}{not defined}\\\hline
 \end{tabular}
 }
\label{Paramtable3}
\end{table}
%\multirow{2}{*}{$|\frac{J(Iter+1)-J(Iter)}{J(Iter+1)}|<10^{-6}$}\begin{tabular}[c]{@{}c@{}}Stopping criterion\\($Iter$ = Iteration number)\end{tabular}

Figures \ref{propgcmmu} and \ref{propcdsu} illustrate the proportion maps resulting from GC-MMU and CDSU respectively. The proportions were multiplied by the corresponding cluster memberships in order to highlight pixels from that cluster. The values are shown as a heat map where small values are in dark blue and large values in dark red. Each proportion map is labeled with the dominant material it represents. In absence of ground truth, endmember labeling was done by comparing the proportion maps to the RGB image in figure \ref{fig1}.
\begin{figure}
\centering
\includegraphics[width=1\linewidth]{prop.png}
\caption{Proportion maps from GC-MMU. Rows correspond to clusters and columns correspond to endmembers.}
\label{propgcmmu}
\end{figure}
\begin{figure}
\centering
\includegraphics[width=1\linewidth]{propcdsu.png}
\caption{Proportion maps from CDSU. Rows correspond to clusters and columns correspond to endmembers.}
\label{propcdsu}
\end{figure}

We can notice that cluster 2 from GC-MMU (2$^{\text{nd}}$ row in figure \ref{propgcmmu}) and cluster 1 from CDSU (1$^{\text{st}}$ row in figure \ref{propcdsu}) are identical. However, the two remaining clusters are different.
CDSU combined bare soil and cement in one endmember in cluster 3 (3$^{\text{rd}}$ row in figure \ref{propcdsu}), whereas GC-MMU succeeded in identifying different endmembers for those materials (3$^{\text{rd}}$ row in figure \ref{propgcmmu}).

We also notice that GC-MMU combined man-made materials (asphalt, bitumen, cement) in one cluster (3$^{\text{rd}}$ row in figure \ref{propgcmmu}), as opposed to CDSU which divided them into two different clusters (2$^{\text{nd}}$ and 3$^{\text{rd}}$ rows in figure \ref{propcdsu}).

We also provide a comparison of running CDSU, GK-CDSU and GC-MMU using the Euclidean and the Mahalanobis distances by reporting the number of satisfied constraints in table \ref{comparetable}.
\begin{table}[htb]
\caption{Number of satisfied constraints using GC-MMU and CDSU with Euclidean and Mahalanobis distances}
\centering
\scalebox{1}{
\begin{tabular}{|c|c|c|c|}
   \hline
   \multicolumn{2}{|c|}{\textbf{Semi-supervised}}&\multicolumn{2}{|c|}{\textbf{Unsupervised}}\\\hline
   GC-MMU (Euclidean)& GC-MMU (Mahalanobis) & CDSU & GK-CDSU\\\hline
   4842 / 6840& \textbf{6591} / 6840 & 4755 / 6840&6263 / 6840\\\hline
 \end{tabular}
 }
\label{comparetable}
\end{table}
We notice that running GC-MMU using the Mahalanobis distance yields more satisfied constraints. This is expected since the Mahalanobis distance allows for more degrees of freedom in order for the constraints from semi-supervision to have an impact, unlike the Euclidean distance which constrains the shape of the clusters to spheres. It is to mention here that the proportion maps for GK-CDSU and GC-MMU with the Mahalanobis distance are fairly similar for the naked eye. Moreover, the proportion maps for CDSU and GC-MMU using the Euclidean distance are also fairly similar for the naked eye. This proves that the Mahalanobis distance is more influential than the semi-supervision information for the clusters formation.
\section{Robust Context Dependent Spectral Unmixing}
\label{sec:exprcdsu}
We design the following experiment so that we evaluate the ability of RCDSU to handle noisy data. We use a simulated hyperspectral data set with different noise levels. The parameters $\eta_{i}$ are updated every iteration using \cite{KrishnaPCM}
\begin{equation}\label{eta}
    \eta_{i}=mean\{cost_{ij},\; t_{ij}\geq t_{i}^{Q}\},
\end{equation}
where $cost_{ij}$ is given by (\ref{costt}), and $t_{i}^{Q}$ is the $Q^{\text{th}}$ percentile of the typicalities in cluster $i$. $Q$ can be thought of as the percentage of points not belonging to cluster $i$. In this experiment, we report the results when $Q=100-\frac{100}{C}$.
Furthermore, the convergence of the algorithm is checked by comparing the values of the objective function from successive iterations. If the difference is below some threshold, the algorithm is stopped.

We generate a data having two convex sets. Each convex set is generated using the convex geometry model in (\ref{convmodl}), using three different endmembers, selected from the USGS (United States Geological Survey) digital spectral library \cite{75}. The six selected endmembers correspond to the minerals \emph{Spessartine}, \emph{Halloysite}, \emph{Chlorite}, \emph{Rectorite}, \emph{Kaolinite} and \emph{Teepleite}. We generate $1000$ spectral signatures in each set. The spectra have $222$ spectral bands, spanning the $0.4-2.5$ $\mu m$ wavelength range. The proportions were randomly generated from a standard uniform distribution and were normalized to sum to one.

We randomly select few points from each set and add a zero-mean Gaussian noise to them, at a signal to noise ratio (SNR) of $1$ dB. We experiment with noise levels of 0 \%, 5\% and 10\%.

We run PCOMMEND \cite{48}, GK-CDSU and RCDSU 25 times using the parameters in table \ref{Paramtable5}. When noise is added, we set a higher weight to the possibilistic memberships ($b=0.9$) compared to the fuzzy memberships ($a=0.1$). Otherwise, we set $a=0.9$ and $b=0.1$.%For both algorithms, we specify the correct number of contexts $C=2$.%For the U-RCDSU, we let $C_{max}=5$, and the algorithm converged to $C=2$.
\begin{table}[htb]
\caption{Parameters used for PCOMMEND, GK-CDSU and RCDSU}
\centering
\scalebox{1}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
   \hline
   % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
    & $C$ &$M$ &$m$& $n$&$a$&$b$&$\beta_{i},\; \forall i$&$\sigma_{i},\; \forall i$&$\alpha$\\\hline
   \textbf{PCOMMEND}& 2 & 3 & 1.5& \multicolumn{5}{c|}{not defined}&1 \\\hline
   \textbf{GK-CDSU}& 2 & 3 & 1.5& \multicolumn{3}{c|}{not defined}&1 &1&100\\\hline
   \textbf{RCDSU} & 2 & 3 & 1.5&1.5 &0.1&0.9& 1&1& 100\\\hline
   %U-RCDSU & 5 & 3 & 1.5&1.5&0.5&0.5& 10& 5\\\hline
 \end{tabular}
 }
\label{Paramtable5}
\end{table}

The estimated endmembers from all algorithms are compared to the true endmembers using the spectral mean angle error (SMAE) defined in equation (\ref{SMAE}). We report the average and standard deviation of the SMAE of the resulting endmember estimates from all algorithms in all runs in table \ref{SMAEtable}. A two-sample t-test at the 5\% significance level shows that, for noisy data, RCDSU provides significantly better endmember estimates than PCOMMEND and GK-CDSU ($p$-value $< 10^{-20}$).
\begin{table}[htb]
\caption{Average ($\pm$ standard deviation) of the SMAE over 25 runs for PCOMMEND, GK-CDSU and RCDSU}
\centering
\scalebox{1}{
\begin{tabular}{|c|c|c|c|c|}
   \hline
   % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
    \textbf{\% of noise points}&\textbf{0}& \textbf{5}&\textbf{10}\\\hline
   \textbf{PCOMMEND} & 0.0630 ($\pm$0)&0.1609 ($\pm$0.0046) &0.1644 ($\pm$0.0135)\\\hline
   \textbf{GK-CDSU}& 0.0630 ($\pm$0)&0.1609 ($\pm$0.0046) &0.1662 ($\pm$0.0119)\\\hline%\textbf{CDSU}& 0.1634 ($\pm$0.0045)&0.1649 ($\pm$0.0035) &0.1608 ($\pm$0.0163)\\\hline%\textbf{CDSU}& 0.1391&0.1569 &0.1585\\\hline
   \textbf{RCDSU} & 0.0639 ($\pm$0)&0.0681 ($\pm$0.0074) &0.0757 ($\pm$0.0105)\\\hline%\textbf{RCDSU} & 0.0662 ($\pm$0.0008)&0.0645 ($\pm$0.0022) &0.0720 ($\pm$0.0023)\\\hline%\textbf{RCDSU} & 0.0847&0.0811 &0.0743\\\hline
   %\textbf{U-RCDSU }&0.0708 &0.0751 &0.0954\\\hline
 \end{tabular}
 }
\label{SMAEtable}
\end{table}
We illustrate the estimated (dashed line) versus the true (solid line) endmembers in figure \ref{simu}. We can see that RCDSU was not influenced by the presence of the noise points unlike GK-CDSU which got affected and resulted in erroneous estimates.
\begin{figure}
\begin{subfigure}[b]{1\linewidth}
  \centering
  \includegraphics[width=0.9\linewidth, height=3in]{CDSUSimulatedSNR1TruevsEstimated.png}%
  \caption{GK-CDSU}
  \label{CDSUsimul}
\end{subfigure}

\begin{subfigure}[b]{1\linewidth}
  \centering
  \includegraphics[width=0.9\linewidth, height=3in]{RCDSUSimulatedSNR1TruevsEstimated.png}%[width=1\linewidth, height=1.5in]
  \caption{RCDSU}
  \label{RCDSUsimu}
\end{subfigure}
\caption{True (solid line) and estimated (dashed line) endmembers with 5\% noise points.}
\label{simu}
\end{figure}

To verify the ability of RCDSU to identify noise points, after convergence, we identified points with small ($<0.1$) possibilistic memberships in all clusters. All of these points correspond to the points with added noise.
\section{Unsupervised Robust Context Dependent Spectral Unmixing}
In this section, we evaluate the ability of U-RCDSU to determine the correct number of contexts using simulated data, then we try it on a real data.
\subsection{Synthetic data}
We use the same data of section \ref{sec:exprcdsu} and we run U-RCDSU by overspecifying the number of clusters to $C_{max}=5$. The other parameters are the same as in section \ref{sec:exprcdsu}. The algorithm converged to $C=2$. The resulting endmembers are shown in figure \ref{URCDSUsimu}. We can see that they are unaffected by the noise points, and are similar to the results of RCDSU in figure \ref{RCDSUsimu}.
\begin{figure}
  \centering
  \includegraphics[width=0.9\linewidth, height=3in]{ERCDSUSimulatedSNR1TruevsEstimated.png}%
  \caption{True (solid line) and estimated (dashed line) endmembers with 5\% noise points using U-RCDSU.}
  \label{URCDSUsimu}
\end{figure}
\subsection{Real data}
\subsubsection{Pavia University data}
We use the Pavia University data presented in section \ref{sec:expcdsu}. We run the unsupervised RCDSU algorithm using $C_{max}=10$, $M=3$, $m=n=1.5$, $a=b=0.5$, $\alpha=100$, $\beta_{i}=4,\; \forall i$, $\sigma_{i}=1,\; \forall i$ and $\epsilon=0.1$. The algorithm converged to $C=2$. Since the ground truth is not available for the entire scene, we evaluate the performance of the algorithm qualitatively by displaying the resulting proportion maps and interpreting them based on the RGB image of the scene shown in figure \ref{pavia}.

The proportion maps associated with the three endmembers for each of the two clusters are shown in figure \ref{propmapur}. The proportions were multiplied by the corresponding weighted cluster memberships ($au_{ij}+bt_{ij}$) in order to highlight pixels from that cluster. Dark blue represents small values and dark red represents large values. Each proportion map is labeled with the dominant material it represents.

It can be seen that U-RCDSU resulted in two intuitive clusters in the sense that one cluster corresponds to natural materials, and the other corresponds to man-made materials. The three proportion maps of context 1 (figure \ref{clus1ur}) correspond to vegetation, shadows and bare soil, respectively. These materials represent natural regions in the scene. The three proportion maps of context 2 (figure \ref{clus2ur}) correspond to brick roofs, metal roofs, and asphalt and bitumen, respectively. They represent urban regions in the scene. One may conclude that U-RCDSU resulted in coherent contexts with appropriate endmembers within each context.
\begin{figure}
\begin{subfigure}[b]{1\linewidth}
\centering
\includegraphics[width=0.9\linewidth,height=0.6\linewidth]{FinalFinalPropMapPaviaaUbTcontext1.png}%
\caption{Proportion maps in context 1}
  \label{clus1ur}
\end{subfigure}
\begin{subfigure}[b]{1\linewidth}
\centering
\includegraphics[width=0.9\linewidth,height=0.6\linewidth]{FinalFinalPropMapPaviaaUbTcontext2.png}%
\caption{Proportion maps in context 2}
  \label{clus2ur}
\end{subfigure}
\caption{Proportion maps resulting from U-RCDSU.}
\label{propmapur}
\end{figure}
\subsubsection{University of Southern Mississippi data}
We use the University of Southern Mississippi data presented in section \ref{sec:gcmmu} without dimensionality reduction. We run the unsupervised RCDSU algorithm using $C_{max}=5$, $M=3$, $m=n=1.5$, $a=b=0.5$, $\alpha=100$, $\beta_{i}=1,\; \forall i$, $\sigma_{i}=1,\; \forall i$ and $\epsilon=0.2$. The algorithm converged to $C=2$. Since the ground truth is not available for the entire scene, we evaluate the performance of the algorithm qualitatively by displaying the resulting proportion maps and interpreting them based on the RGB image of the scene shown in figure \ref{rgb}. Each proportion map was multiplied by the corresponding weighted memberships ($au_{ij}+bt_{ij}$). Figure \ref{propurcdsumissiissippi} illustrates these maps where the title of each one represents the corresponding dominant materials.
\begin{figure}
\centering
\includegraphics[width=1\linewidth]{URCDSUMississippiResults.png}
\caption{Proportion maps from U-RCDSU on the Mississippi data. Rows correspond to clusters and columns correspond to endmembers.}
\label{propurcdsumissiissippi}
\end{figure}

The endmembers in cluster 1 correspond to grass, asphalt and bitumen, and beach sand. In cluster 2, the endmembers represent bare soil, shadows and trees.
\section{Context Dependent Hyperspectral Subpixel Target Detection}
We designed these experiments to evaluate the performance of the proposed context dependent (CD) target detectors compared to the traditional detectors that use a single endmember set to describe the background. For the traditional detectors, we use the MVSA, NFINDR, PPI and the eigenvectors of the data correlation matrix method, to find a set of $3$ or $6$ endmembers (referred to as MVSA3 and MVSA6, NFINDR3 and NFINDR6, PPI3 and PPI6, and EigVect3 and EigVect6 respectively).
\subsection{Data with implanted targets}
We use the Indian Pines data \cite{indianpine} which consists of $145\times 145$ pixels having $200$ spectral bands each, covering the $0.4-2.5 \mu m$ wavelength range. We implant one hundred spectral signatures of a red tarp target with abundance values ranging from $0.1$ to $1$. Figure \ref{band115} shows band 115 ($\sim 1.5\mu m$) of the image where we can see the grid of $100$ targets. The top row consists of the targets with abundance values of $0.1$. These are difficult to see since they are close to the background. The next row consists of the targets with abundance values of $0.2$. This pattern continues till the last row which corresponds to pure targets of abundance values of $1$.
\begin{figure}
\centering
\includegraphics[width=0.5\linewidth,height=0.5\linewidth]{band115.png}%
\caption{Band 115 of the image with $100$ implanted targets.}
\label{band115}
\end{figure}
We run U-RCDSU using $C_{max}=5$, $M=3$, $m=n=1.5$, $a=0.1$, $b=0.9$, $\alpha=100$, $\beta_{i}=1,\; \forall i$, $\sigma_{i}=1,\; \forall i$ and $\epsilon=0.1$. The algorithm converges to $C=2$ clusters.

We start by analyzing the resulting possibilistic memberships of the target pixels in both clusters. Figure \ref{scatpossitargsize} shows a scatter plot of the maximum possibilistic memberships of the targets in both clusters as a function of their proportions in the pixels. All proportions are very small ($< 0.06$), which means that the targets did not contribute to the estimated local background subspaces. In other words, there was no leakage of targets into the endmember sets. We can also notice that the larger the proportion of the target, the smaller its possibilistic membership, which is expected.
\begin{figure}
\centering
  \includegraphics[width=0.7\linewidth]{scatterTargetsizePossibilisticICIP.png}\\
  \caption{Scatter plot of the possibilistic memberships of the targets as a function of their proportions in the pixels}
  \label{scatpossitargsize}
\end{figure}
These memberships could be used on their own to detect targets. However, this would result in a large number of false alarms, since the memberships cannot discriminate between targets and non-target outliers.

We scatter plot the detection statistic of CD-OSP as a function of the target proportion in the pixel. This is illustrated in figure \ref{scatcdospscoretargsize}. We can notice that the statistic increases as the target proportion increases, which is expected.
\begin{figure}
\centering
  \includegraphics[width=0.7\linewidth]{scatterTargetsizecdospScoreICIP.png}\\
  \caption{Scatter plot of the CD-OSP statistic as a function of the target proportion in the pixel}
  \label{scatcdospscoretargsize}
\end{figure}

The proportion values in (\ref{hsd}) are computed using the endmember sets resulting from MVSA with the proportions' update equation of CDSU.
%whether in the background subspace or in the composite target and background space,, by plugging the subspace matrix into consideration (resulting from MVSA).
The proportion values $\mathbf{p}_{\mathbf{b}ij}$ in (\ref{rcdsuhsd}) are the results of U-RCDSU (same as $\mathbf{p}_{ij}$ in (\ref{ObjFuncFuzzPoss})). The proportion values $\mathbf{p}_{\mathbf{s}ij}$, on the other hand, are computed using the composite target and background spaces with the proportions' update equation of CDSU.

The performance of the detectors is evaluated using the receiver operating characteristic (ROC) curves. In figure \ref{fig:res}, we plot the ROC curves of the proposed context dependent detectors versus those of the traditional detectors using MVSA.
\begin{figure}
\begin{subfigure}[b]{0.49\linewidth}
  \centering
\includegraphics[width=1\linewidth,height=0.8\linewidth]{realimposp.png}
  \caption{OSP detector}\medskip
\end{subfigure}
\begin{subfigure}[b]{.49\linewidth}
  \centering
  \includegraphics[width=1\linewidth,height=0.8\linewidth]{realimpasmsd.png}
  \caption{AMSD detector}\medskip
\end{subfigure}
\hfill
\begin{subfigure}[b]{1\linewidth}
  \centering
  \includegraphics[width=0.49\linewidth,height=0.4\linewidth]{realimphsd.png}
  \caption{HSD detector}\medskip
\end{subfigure}
\caption{Receiver operating characteristic (ROC) curves.}
\label{fig:res}
\end{figure}
It can be seen from figure \ref{fig:res} that the proposed context dependent target detectors outperform the traditional MVSA single subspace detectors. We also evaluate the performance of the detectors using the Area Under Curve (AUC) which measures the area under the ROC curves. Table \ref{AUCtable} reports the average and standard deviation of this measure for the CD, EigVect, MVSA, NFINDR and PPI approaches using the OSP, AMSD and HSD detectors over 25 runs.
\begin{table}
\caption{Average ($\pm$ standard deviation) of the AUC over 25 runs for the CD, EigVect, MVSA, NFINDR and PPI methods (Indian Pine data)}
\centering
\scalebox{1}{
\begin{tabular}{|c|c|c|c|}
   \hline
   \textbf{Detector}&\textbf{OSP}&\textbf{AMSD}&\textbf{HSD}\\\hline
   \textbf{CD}&\textbf{0.9921} ($\pm$0.0000)&\textbf{0.9443} ($\pm$0.0507)& 0.9977 ($\pm$0.0000)\\\hline
   \textbf{EigVect3}&0.9822 ($\pm$0.0000) &0.8916 ($\pm$0.0000) &\textbf{0.9978} ($\pm$0.0000)\\\hline
   \textbf{EigVect6}&0.9822 ($\pm$0.0000)&0.8916 ($\pm$0.0000)&\textbf{0.9978} ($\pm$0.0000)\\\hline
   \textbf{MVSA3}&0.9426 ($\pm$0.0000)&0.8928 ($\pm$0.0500)&0.9725 ($\pm$0.0000)\\\hline
   \textbf{MVSA6}&0.9656 ($\pm$0.0000)&0.8992 ($\pm$0.0476)&0.9537 ($\pm$0.0001)\\\hline
   \textbf{NFINDR3}&0.8270 ($\pm$0.0566)& 0.4652 ($\pm$0.1206)&0.6976 ($\pm$0.0006)\\\hline
   \textbf{NFINDR6}&0.6502 ($\pm$0.4482)&0.5316 ($\pm$0.3951)&0.7417 ($\pm$0.0368)\\\hline
   \textbf{PPI3}&0.3911 ($\pm$0.3438)&0.5000 ($\pm$0.0000)&0.1792 ($\pm$0.0590)\\\hline
   \textbf{PPI6}&0.5314 ($\pm$0.4465)&0.5313 ($\pm$0.3947)&0.3357 ($\pm$0.2581)\\\hline
 \end{tabular}
 }
\label{AUCtable}
\end{table}

Using a two-sample t-test at the 5\% significance level, we conclude that the proposed context dependent detection approach outperformed the traditional detection approaches when the OSP and AMSD detectors were used ($p-value<0.003$). For HSD, the correlation matrix eigen vectors based method outperformed the rest of the methods ($p-value<1e-10$). However, the context dependent approach still outperformed the other endmember detection methods. This can be explained by the better description of the background using the RCDSU algorithm, which also ensures that there is no leakage of targets into the endmember sets using the possibilistic memberships. We also notice the robustness of the context dependent approach to initialization (AUC standard deviation $<0.05$).
\subsection{Data with actual targets}
The University of Southern Mississippi data presented in Section \ref{sec:gcmmu} presents different targets with different sizes. Table \ref{mississippitargets} reports the number of targets from each type and each size.
\begin{table}
\caption{Number of targets from each type and each size in the Mississippi data}
\centering
\scalebox{1}{
\begin{tabular}{|l|c|c|c|c|}
  \hline
  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
  \backslashbox{\textbf{Type}}{\textbf{Size in pixels}} & \textbf{0.5} & \textbf{1} & \textbf{3} & \textbf{Total}\\\hline
  Brown & 5 & 5 & 5 & 15\\\hline
  Dark green & 5 & 5 & 5 & 15 \\\hline
  Faux vineyard green & 4 & 4 & 4 & 12 \\\hline
  Pea green & 5 & 5 & 5 & 15\\
  \hline
\end{tabular}
 }
\label{mississippitargets}
\end{table}
We run U-RCDSU using $C_{max}=5$, $M=3$, $m=n=1.5$, $a=0.1$, $b=0.9$, $\alpha=100$, $\beta_{i}=4,\; \forall i$, $\sigma_{i}=1,\; \forall i$ and $\epsilon=0.1$. The algorithm converges to $C=2$ clusters.

We evaluate the performance of the detection methods using the area under curve measure. Table \ref{AUCtablemississpibrown} reports this measure for all methods for the ``brown'' target. Two cases are analyzed: all target sizes combined and subpixel targets only. For all target sizes combined, we can see that the proposed context dependent approach gave good AUCs, outperforming the other methods when the AMSD and HSD detectors were used ($p-value<0.02$). For the subpixel targets, the context dependent target detectors gave the best AUC when used with HSD ($p-value<0.05$).
\begin{table}
\caption{Average ($\pm$ standard deviation) of the AUC over 25 runs for the CD, EigVect, MVSA, NFINDR and PPI methods (Mississippi data, brown target)}
\centering
\scalebox{1}{
\begin{tabular}{|c|c|c|c|}
   \hline
   \textbf{Detector}&\textbf{OSP}&\textbf{AMSD}&\textbf{HSD}\\\hline
   \textbf{AUC for all target sizes}&\multicolumn{3}{c|}{}\\\hline
   CD&0.9657 ($\pm$0.0000)&\textbf{0.9812} ($\pm$0.0000)& \textbf{0.9778} ($\pm$0.0000)\\\hline
   EigVect3&0.8665 ($\pm$0.0000) &0.9365 ($\pm$0.0000) &0.9107 ($\pm$0.0000)\\\hline
   EigVect6&0.9665 ($\pm$0.0000)&0.9365 ($\pm$0.0000)&0.9107 ($\pm$0.0000)\\\hline
   MVSA3&0.9652 ($\pm$0.0000)&0.9788 ($\pm$0.0000)&0.9671 ($\pm$0.0214)\\\hline
   MVSA6&\textbf{0.9664} ($\pm$0.0000)&0.9697 ($\pm$0.0000)&0.9429 ($\pm$0.0128)\\\hline
   NFINDR3&0.7892 ($\pm$0.0000)& 0.8328 ($\pm$0.0000)&0.7951 ($\pm$0.0037)\\\hline
   NFINDR6&0.9252 ($\pm$0.0257)&0.9359 ($\pm$0.0186)&0.8580 ($\pm$0.0938)\\\hline
   PPI3&0.8505 ($\pm$0.0506)&0.9011 ($\pm$0.0382)&0.8800 ($\pm$0.0086)\\\hline
   PPI6&0.8741 ($\pm$0.0095)&0.9301 ($\pm$0.0058)&0.8709 ($\pm$0.0001)\\\hline
   \textbf{AUC for target size 0.5}&\multicolumn{3}{c|}{}\\\hline
   CD&0.9773 ($\pm$0.0000)&0.9854 ($\pm$0.0000)& \textbf{0.9891} ($\pm$0.0000)\\\hline
   EigVect3&0.9229 ($\pm$0.0000) &0.9795 ($\pm$0.0000) &0.9727 ($\pm$0.0000)\\\hline
   EigVect6&0.9229 ($\pm$0.0000)&0.9795 ($\pm$0.0000)&0.9727 ($\pm$0.0000)\\\hline
   MVSA3&0.9779 ($\pm$0.0000)&0.9846 ($\pm$0.0000)&0.9654 ($\pm$0.0000)\\\hline
   MVSA6&\textbf{0.9817} ($\pm$0.0000)&0.9774 ($\pm$0.0000)&0.9659 ($\pm$0.0001)\\\hline
   NFINDR3&0.8656 ($\pm$0.0000)& 0.8979 ($\pm$0.0000)&0.9823 ($\pm$0.0000)\\\hline
   NFINDR6&0.9765 ($\pm$0.0173)&\textbf{0.9884} ($\pm$0.0072)&0.9190 ($\pm$0.1225)\\\hline
   PPI3&0.8973 ($\pm$0.0424)&0.9243 ($\pm$0.0509)&0.9414 ($\pm$0.0070)\\\hline
   PPI6&0.8805 ($\pm$0.0151)&0.9723 ($\pm$0.0052)&0.9318 ($\pm$0.0009)\\\hline
 \end{tabular}
 }
\label{AUCtablemississpibrown}
\end{table}

Table \ref{AUCtablemississpidarkgreen} reports this measure for all methods for the ``dark green'' target, when all target sizes are considered and when subpixel targets only are considered. For all target sizes combined, we can see that the proposed context dependent approach gave good AUCs, outperforming the other methods when the OSP and HSD detectors were used ($p-value<0.006$). For the subpixel targets, MVSA3 outperformed all other methods ($p-value<0.003$).
\begin{table}
\caption{Average ($\pm$ standard deviation) of the AUC over 25 runs for the CD, EigVect, MVSA, NFINDR and PPI methods (Mississippi data, dark green target)}
\centering
\scalebox{1}{
\begin{tabular}{|c|c|c|c|}
   \hline
   \textbf{Detector}&\textbf{OSP}&\textbf{AMSD}&\textbf{HSD}\\\hline
   \textbf{AUC for all target sizes}&\multicolumn{3}{c|}{}\\\hline
   CD&\textbf{0.9533} ($\pm$0.0000)&0.9760 ($\pm$0.0000)& \textbf{0.9781} ($\pm$0.0000)\\\hline
   EigVect3&0.9171 ($\pm$0.0000) &0.9251 ($\pm$0.0000) &0.9341 ($\pm$0.0000)\\\hline
   EigVect6&0.9171 ($\pm$0.0000)&0.9251 ($\pm$0.0000)&0.9341 ($\pm$0.0000)\\\hline
   MVSA3&0.9510 ($\pm$0.0000)&\textbf{0.9795} ($\pm$0.0000)&0.9558 ($\pm$0.0000)\\\hline
   MVSA6&0.9149 ($\pm$0.0000)&0.9518 ($\pm$0.0000)&0.9165 ($\pm$0.0000)\\\hline
   NFINDR3&0.8874 ($\pm$0.0000)& 0.8649 ($\pm$0.0000)&0.8676 ($\pm$0.0000)\\\hline
   NFINDR6&0.9088 ($\pm$0.0202)&0.9224 ($\pm$0.0161)&0.8370 ($\pm$0.0392)\\\hline
   PPI3&0.9418 ($\pm$0.0199)&0.9672 ($\pm$0.0040)&0.9189 ($\pm$0.0173)\\\hline
   PPI6&0.9179 ($\pm$0.0143)&0.9569 ($\pm$0.0089)&0.8978 ($\pm$0.0030)\\\hline
   \textbf{AUC for target size 0.5}&\multicolumn{3}{c|}{}\\\hline
   CD&0.9620 ($\pm$0.0000)&0.9679 ($\pm$0.0000)& 0.9829 ($\pm$0.0000)\\\hline
   EigVect3&0.9132 ($\pm$0.0000) &0.9693 ($\pm$0.0000) &0.9765 ($\pm$0.0000)\\\hline
   EigVect6&0.9132 ($\pm$0.0000)&0.9693 ($\pm$0.0000)&0.9765 ($\pm$0.0000)\\\hline
   MVSA3&\textbf{0.9634} ($\pm$0.0000)&\textbf{0.9758} ($\pm$0.0000)&\textbf{0.9858} ($\pm$0.0000)\\\hline
   MVSA6&0.9309 ($\pm$0.0000)&0.9493 ($\pm$0.0000)&0.9453 ($\pm$0.0000)\\\hline
   NFINDR3&0.9342 ($\pm$0.0000)& 0.9197 ($\pm$0.0000)&0.9397 ($\pm$0.0000)\\\hline
   NFINDR6&0.9399 ($\pm$0.0065)&0.9467 ($\pm$0.0088)&0.8907 ($\pm$0.1154)\\\hline
   PPI3&0.9629 ($\pm$0.0028)&0.9737 ($\pm$0.0086)&0.9485 ($\pm$0.0019)\\\hline
   PPI6&0.9515 ($\pm$0.0148)&0.9724 ($\pm$0.0012)&0.9468 ($\pm$0.0011)\\\hline
 \end{tabular}
 }
\label{AUCtablemississpidarkgreen}
\end{table}

Table \ref{AUCtablemississpifauxvineyardgreen} reports this measure for all methods for the ``faux vineyard green'' target, when all target sizes are considered and when subpixel targets only are considered. For all target sizes combined, we can see that the proposed context dependent approach gave good AUCs, outperforming the other methods when the HSD detector is used ($p-value<0.005$). However, MVSA6 with AMSD gave the highest AUC. For subpixel targets, MVSA3 with HSD gave the highest AUC ($p-value<0.003$).
\begin{table}
\caption{Average ($\pm$ standard deviation) of the AUC over 25 runs for the CD, EigVect, MVSA, NFINDR and PPI methods (Mississippi data, faux vineyard green target)}
\centering
\scalebox{1}{
\begin{tabular}{|c|c|c|c|}
   \hline
   \textbf{Detector}&\textbf{OSP}&\textbf{AMSD}&\textbf{HSD}\\\hline
   \textbf{AUC for all target sizes}&\multicolumn{3}{c|}{}\\\hline
   CD&0.9328 ($\pm$0.0000)&0.9439 ($\pm$0.0000)& \textbf{0.9695} ($\pm$0.0000)\\\hline
   EigVect3&0.9078 ($\pm$0.0000) &0.8917 ($\pm$0.0000) &0.8416 ($\pm$0.0000)\\\hline
   EigVect6&0.9078 ($\pm$0.0000)&0.8917 ($\pm$0.0000)&0.8416 ($\pm$0.0000)\\\hline
   MVSA3&0.9317 ($\pm$0.0000)&0.9378 ($\pm$0.0000)&0.9639 ($\pm$0.0000)\\\hline
   MVSA6&0.9448 ($\pm$0.0000)&\textbf{0.9791} ($\pm$0.0000)&0.9394 ($\pm$0.0000)\\\hline
   NFINDR3&0.8821 ($\pm$0.0000)& 0.8408 ($\pm$0.0000)&0.8825 ($\pm$0.0000)\\\hline
   NFINDR6&\textbf{0.9493} ($\pm$0.0160)&0.9523 ($\pm$0.0226)&0.9246 ($\pm$0.0714)\\\hline
   PPI3&0.8180 ($\pm$0.0659)&0.9672 ($\pm$0.0040)&0.8134 ($\pm$0.0533)\\\hline
   PPI6&0.9405 ($\pm$0.0034)&0.9569 ($\pm$0.0089)&0.9056 ($\pm$0.0016)\\\hline
   \textbf{AUC for target size 0.5}&\multicolumn{3}{c|}{}\\\hline
   CD&0.9668 ($\pm$0.0000)&0.9708 ($\pm$0.0000)& 0.9891 ($\pm$0.0000)\\\hline
   EigVect3&0.9692 ($\pm$0.0000) &0.9390 ($\pm$0.0000) &0.9211 ($\pm$0.0000)\\\hline
   EigVect6&0.9692 ($\pm$0.0000)&0.9390 ($\pm$0.0000)&0.9211 ($\pm$0.0000)\\\hline
   MVSA3&0.9678 ($\pm$0.0000)&0.9704 ($\pm$0.0000)&\textbf{0.9893} ($\pm$0.0000)\\\hline
   MVSA6&0.9585 ($\pm$0.0000)&\textbf{0.9792} ($\pm$0.0000)&0.9428 ($\pm$0.0000)\\\hline
   NFINDR3&0.9376 ($\pm$0.0000)& 0.9038 ($\pm$0.0000)&0.9679 ($\pm$0.0000)\\\hline
   NFINDR6&0.9677 ($\pm$0.0141)&0.9706 ($\pm$0.0174)&0.9842 ($\pm$0.0072)\\\hline
   PPI3&0.8465 ($\pm$0.0646)&0.8363 ($\pm$0.0242)&0.8937 ($\pm$0.0529)\\\hline
   PPI6&\textbf{0.9727} ($\pm$0.0012)&0.9186 ($\pm$0.0001)&0.9362 ($\pm$0.0006)\\\hline
 \end{tabular}
 }
\label{AUCtablemississpifauxvineyardgreen}
\end{table}

Table \ref{AUCtablemississpipeagreen} reports this measure for all methods for the ``faux vineyard green'' target, when all target sizes are considered and when subpixel targets only are considered. When all target sizes are considered, we can see that the proposed context dependent approach gave good AUCs, however not as good as the MVSA3 with AMSD and HSD, and PPI6 with OSP ($p-value<1e-9$). For subpixel targets, the context dependent target detection yielded the highest AUC when HSD was used ($p-value<0.01$).
\begin{table}
\caption{Average ($\pm$ standard deviation) of the AUC over 25 runs for the CD, EigVect, MVSA, NFINDR and PPI methods (Mississippi data, pea green target)}
\centering
\scalebox{1}{
\begin{tabular}{|c|c|c|c|}
   \hline
   \textbf{Detector}&\textbf{OSP}&\textbf{AMSD}&\textbf{HSD}\\\hline
   \textbf{AUC for all target sizes}&\multicolumn{3}{c|}{}\\\hline
   CD&0.9563 ($\pm$0.0000)&0.9672 ($\pm$0.0000)& 0.9693 ($\pm$0.0000)\\\hline
   EigVect3&0.9177 ($\pm$0.0000) &0.7572 ($\pm$0.0000) &0.7230 ($\pm$0.0000)\\\hline
   EigVect6&0.9177 ($\pm$0.0000)&0.7572 ($\pm$0.0000)&0.7230 ($\pm$0.0000)\\\hline
   MVSA3&0.9532 ($\pm$0.0000)&\textbf{0.9675} ($\pm$0.0000)&\textbf{0.9748} ($\pm$0.0000)\\\hline
   MVSA6&0.9663 ($\pm$0.0000)&0.9245 ($\pm$0.0000)&0.9480 ($\pm$0.0000)\\\hline
   NFINDR3&0.8604 ($\pm$0.0000)& 0.8264 ($\pm$0.0000)&0.7816 ($\pm$0.0000)\\\hline
   NFINDR6&0.9582 ($\pm$0.0249)&0.9426 ($\pm$0.0113)&0.6097 ($\pm$0.1712)\\\hline
   PPI3&0.8200 ($\pm$0.0480)&0.8519 ($\pm$0.0557)&0.7358 ($\pm$0.0348)\\\hline
   PPI6&\textbf{0.9657} ($\pm$0.0003)&0.9281 ($\pm$0.0002)&0.7126 ($\pm$0.0000)\\\hline
   \textbf{AUC for target size 0.5}&\multicolumn{3}{c|}{}\\\hline
   CD&0.9741 ($\pm$0.0000)&\textbf{0.9472} ($\pm$0.0000)& \textbf{0.9769} ($\pm$0.0000)\\\hline
   EigVect3&0.9524 ($\pm$0.0000) &0.7841 ($\pm$0.0000) &0.7900 ($\pm$0.0000)\\\hline
   EigVect6&0.9524 ($\pm$0.0000)&0.7841 ($\pm$0.0000)&0.7900 ($\pm$0.0000)\\\hline
   MVSA3&0.9729 ($\pm$0.0000)&0.9544 ($\pm$0.0000)&0.9735 ($\pm$0.0000)\\\hline
   MVSA6&0.9663 ($\pm$0.0001)&0.8925 ($\pm$0.0000)&0.9469 ($\pm$0.0001)\\\hline
   NFINDR3&0.9344 ($\pm$0.0000)& 0.8400 ($\pm$0.0000)&0.7774 ($\pm$0.0000)\\\hline
   NFINDR6&0.9672 ($\pm$0.0224)&0.9351 ($\pm$0.0223)&0.7299 ($\pm$0.1899)\\\hline
   PPI3&0.8586 ($\pm$0.0790)&0.8628 ($\pm$0.0450)&0.7956 ($\pm$0.0158)\\\hline
   PPI6&\textbf{0.9766} ($\pm$0.0002)&0.9316 ($\pm$0.0002)&0.7892 ($\pm$0.0001)\\\hline
 \end{tabular}
 }
\label{AUCtablemississpipeagreen}
\end{table}

We can also notice that the context dependent target detection approach is robust to initialization (very small standard deviation).
%\\The next chapter provides conclusions for this proposal and presents the proposed future work. 