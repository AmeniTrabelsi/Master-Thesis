\chapter{Conclusions and Proposed Future Work}
\section{Conclusions}
In this thesis, we have introduced a new approach that aims to accelerate data-driven discovery of materials property. We developed statistical learning algorithms supervised by fundamental materials science principals to predict and model two key properties of different types of semiconductors. We focused on two main components of the machine learning process: (i) feature extraction and selection; and (ii) learning algorithms. We combined different feature selection techniques to Lasso, Ordinary Least Square and Partial Least Square regressions to build robust regression models that takes into account the intrinsic properties of the training data as well as the impacts of the different descriptors.\\
\indent Our approach was successfully applied and tested to enhance the prediction of chalcopyrites' band gap, and to identify the top factors responsible for Defect-Induced Magnetism in Dilute Semiconductors by predicting several systems' total magnetic moment. 
\\ \indent We have significantly improved upon prior results in informatics-based prediction of band gap by reducing the dimensionality of the training data. Our experiments showed that our approach can boost band gap prediction accuracy by more than $40\%$ for the same testing set while keeping a high consistency between the different regression techniques. Furthermore, we found that the band gap energy of $ABC_2$ compounds, is highly correlated to the atomic number and the pseudopotential radius of the B and C elements and to the melting point of the C element. We proved also that the bond dissociation (BD) energy describing B-C and C-C bonds can be a binary feature good candidate to boost our application performance. Combining BD to the previously selected set of features yielded $70\%$ enhancement of the prediction error.\\
\indent For DIM modeling in DMS materials, our approach yielded a number of interesting findings. We showed that the elementary magnetic moment as well as the elementary properties of the first atom in the co-dopant formula and the second atom in the host's are highly correlated to the system's total magnetic moment. This helped us reduce our original set of 30 features to just 8 features while improving the prediction accuracy by around $90\%$.
\section{Proposed Future Work}
Although our approach has shown promising results, there is still room for improvement. The following sections list three main areas that should be explored in the future to build upon the previous work.
\subsection{Expanding the data sets}
Data scarcity has been the main challenge during our work. In the future, we aim to expend the set of labeled testing data for band gap prediction. We will focus mainly on adding more labeled compounds with low confidence regarding the training set.  For DIM modeling in DMSs, our conducted work is still at its preliminary phase. We are still exploring the possibility of expanding our data sets by including new hosts, new features and new co-dopants. Our experiments were carried for two hosts only ($GaN$ and $GaP$). We intend to add more hosts including $ZnO$, $TiO_2$, $MoS_2$ and $SnO_2$. This we help us enormously building more robust models and especially drawing more insightful conclusions. 
\subsection{Clustering}
Generalizing a given regression model is not an easy task, especially when the inherent structure of the data is not favorable. When the available labeled data is limited, which is the case for our applications, applying clustering techniques can be very helpful to perform. It gives more insights about the validity of the generalized model by investigating distributions of both the training and the testing data sets. It helps also detecting outlaying observations prior to the data mining process which allows avoiding performance deterioration due to error propagation and model mis-specification.
Integrating clustering to our learning approach for DMS modeling can be very helpful. In fact, data corresponding to different hosts forms different clusters. We can use this observation to investigate the possibility of building separate regression models for different hosts and even to explore the possibility of integrating multiple instance learning to the process. 
\subsection{Ensemble learning}
Another key area worth investigating in future works in ensemble learning technique. In fact, building a robust regression model that takes into account the intrinsic property of the training data as well as the impacts of the different perturbations it can undergo requires aggregating multiple regression models. So far, we used three different regression techniques and investigated their consistency. However, each technique has its own advantages and flaws depending on the way it considers the inherent data structure.  








